{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: gensim in c:\\users\\abcd\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (4.3.1)Note: you may need to restart the kernel to use updated packages.\n",
      "\n",
      "Requirement already satisfied: numpy>=1.18.5 in c:\\users\\abcd\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from gensim) (1.24.2)\n",
      "Requirement already satisfied: scipy>=1.7.0 in c:\\users\\abcd\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from gensim) (1.10.1)\n",
      "Requirement already satisfied: smart-open>=1.8.1 in c:\\users\\abcd\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from gensim) (6.3.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip available: 22.3.1 -> 23.0.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "pip install gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the libraries \n",
    "import nltk \n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.corpus import stopwords\n",
    "import re\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import numpy as np \n",
    "import gensim\n",
    "from gensim.models import Word2Vec\n",
    "import pandas as pd "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_data = pd.read_csv(\"C:/Users/abcd/Documents/PM_Manmohan_Speech.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>Date</th>\n",
       "      <th>Location</th>\n",
       "      <th>Title</th>\n",
       "      <th>Speech</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>May 17, 2014</td>\n",
       "      <td>New Delhi</td>\n",
       "      <td>PM's address to the Nation</td>\n",
       "      <td>\\n\\tMy Fellow Citizens,\\n\\tI address you today...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>March 4, 2014</td>\n",
       "      <td>Nay Pyi Taw, Myanmar</td>\n",
       "      <td>PM’s statement at 3rd BIMSTEC Summit</td>\n",
       "      <td>\\n\\t“I am delighted to return to this beautifu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>March 1, 2014</td>\n",
       "      <td>New Delhi</td>\n",
       "      <td>PM’s address at the National Awards Ceremony f...</td>\n",
       "      <td>\\n\\t“I am very happy to participate in the Nat...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>February 26, 2014</td>\n",
       "      <td>New Delhi</td>\n",
       "      <td>PM's speech at the foundation laying ceremony ...</td>\n",
       "      <td>\\n\\t“Today marks a very important step forward...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>February 21, 2014</td>\n",
       "      <td>New Delhi</td>\n",
       "      <td>PM’s farewell speech to the 15th Lok Sabha</td>\n",
       "      <td>\"Madam Speaker,\\n\\nAs we come to the end of th...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1396</th>\n",
       "      <td>1396</td>\n",
       "      <td>July 31, 2004</td>\n",
       "      <td>Bangkok</td>\n",
       "      <td>Speech by the Prime Minister of India at the i...</td>\n",
       "      <td>\\n\\t\\t \\n\\tMr. Chairman,\\n\\t \\n\\tExcellencies,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1397</th>\n",
       "      <td>1397</td>\n",
       "      <td>July 26, 2004</td>\n",
       "      <td>New Delhi</td>\n",
       "      <td>PM's speech at the CSIR Society Meeting</td>\n",
       "      <td>Distinguished Members of the CSIR Society, La...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1398</th>\n",
       "      <td>1398</td>\n",
       "      <td>July 3, 2004</td>\n",
       "      <td>New Delhi</td>\n",
       "      <td>PM's inaugural address at the \"Dialogue on Min...</td>\n",
       "      <td>\"It gives me great pleasure to be amongst you...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1399</th>\n",
       "      <td>1399</td>\n",
       "      <td>June 29, 2004</td>\n",
       "      <td>New Delhi</td>\n",
       "      <td>PM's inaugural address at the Chief Ministers'...</td>\n",
       "      <td>\"I welcome you all to this Conference which w...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1400</th>\n",
       "      <td>1400</td>\n",
       "      <td>June 24, 2004</td>\n",
       "      <td>New Delhi</td>\n",
       "      <td>Prime Minister 's Address to the Nation</td>\n",
       "      <td>My Fellow Citizens,I speak to you with a deep...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1401 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      Unnamed: 0               Date              Location   \n",
       "0              0       May 17, 2014             New Delhi  \\\n",
       "1              1      March 4, 2014  Nay Pyi Taw, Myanmar   \n",
       "2              2      March 1, 2014             New Delhi   \n",
       "3              3  February 26, 2014             New Delhi   \n",
       "4              4  February 21, 2014             New Delhi   \n",
       "...          ...                ...                   ...   \n",
       "1396        1396      July 31, 2004               Bangkok   \n",
       "1397        1397      July 26, 2004             New Delhi   \n",
       "1398        1398       July 3, 2004             New Delhi   \n",
       "1399        1399      June 29, 2004             New Delhi   \n",
       "1400        1400      June 24, 2004             New Delhi   \n",
       "\n",
       "                                                  Title   \n",
       "0                            PM's address to the Nation  \\\n",
       "1                  PM’s statement at 3rd BIMSTEC Summit   \n",
       "2     PM’s address at the National Awards Ceremony f...   \n",
       "3     PM's speech at the foundation laying ceremony ...   \n",
       "4            PM’s farewell speech to the 15th Lok Sabha   \n",
       "...                                                 ...   \n",
       "1396  Speech by the Prime Minister of India at the i...   \n",
       "1397            PM's speech at the CSIR Society Meeting   \n",
       "1398  PM's inaugural address at the \"Dialogue on Min...   \n",
       "1399  PM's inaugural address at the Chief Ministers'...   \n",
       "1400            Prime Minister 's Address to the Nation   \n",
       "\n",
       "                                                 Speech  \n",
       "0     \\n\\tMy Fellow Citizens,\\n\\tI address you today...  \n",
       "1     \\n\\t“I am delighted to return to this beautifu...  \n",
       "2     \\n\\t“I am very happy to participate in the Nat...  \n",
       "3     \\n\\t“Today marks a very important step forward...  \n",
       "4     \"Madam Speaker,\\n\\nAs we come to the end of th...  \n",
       "...                                                 ...  \n",
       "1396  \\n\\t\\t \\n\\tMr. Chairman,\\n\\t \\n\\tExcellencies,...  \n",
       "1397   Distinguished Members of the CSIR Society, La...  \n",
       "1398   \"It gives me great pleasure to be amongst you...  \n",
       "1399   \"I welcome you all to this Conference which w...  \n",
       "1400   My Fellow Citizens,I speak to you with a deep...  \n",
       "\n",
       "[1401 rows x 5 columns]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleaned(speech):\n",
    "    # Sentence Tokenization\n",
    "    sentence = nltk.sent_tokenize(speech)\n",
    "\n",
    "    # Creating an empty list to store the cleaned dataset \n",
    "    corpus = []\n",
    "\n",
    "    # Loop through each sentence in the speech\n",
    "    for i in range(len(sentence)):\n",
    "        # Remove any non-alphabetic characters in the sentence and convert to lowercase\n",
    "        review = re.sub(\"[^a-zA-Z0-9]\", \" \", sentence[i])\n",
    "        review = review.lower()\n",
    "        \n",
    "        # Tokenize the sentence into a list of words\n",
    "        review = review.split()\n",
    "        \n",
    "        # Lemmatize each word in the sentence and remove stop words\n",
    "        lemmatizer = WordNetLemmatizer()\n",
    "        review = [lemmatizer.lemmatize(word) for word in review if word not in set(stopwords.words(\"english\"))]\n",
    "        \n",
    "        # Convert the list of words back into a sentence\n",
    "        review = ' '.join(review)\n",
    "        \n",
    "        # Add the cleaned sentence to the corpus\n",
    "        corpus.append(review)\n",
    "\n",
    "    return corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vectorize(corpus):\n",
    "\n",
    "    # Tokenize each sentence in the corpus into a list of words\n",
    "    corpus_tokenized = [nltk.word_tokenize(sentence) for sentence in corpus]\n",
    "\n",
    "    # Train the Word2Vec model on the tokenized corpus\n",
    "    model = Word2Vec(sentences=corpus_tokenized, vector_size=300, window=5, min_count=1, workers=4)\n",
    "\n",
    "    # Extract the word vectors for each word in the corpus\n",
    "    vectors = []\n",
    "    for sentence in corpus_tokenized:\n",
    "        for word in sentence:\n",
    "            if word in model.wv.key_to_index:\n",
    "                # If the word is in the Word2Vec model's vocabulary, extract its vector\n",
    "                word_vector = model.wv.get_vector(word)\n",
    "                vectors.append(np.average(word_vector))\n",
    "            else:\n",
    "                # If the word is not in the vocabulary, print a message to the console\n",
    "                print(f\"{word} not in vocabulary.\") \n",
    "    \n",
    "    return vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To convert cleaned speeches into string for performing NER\n",
    "def string(speech):\n",
    "    string_speech = \" \".join(speech)\n",
    "    return string_speech"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "ename": "LookupError",
     "evalue": "\n**********************************************************************\n  Resource \u001b[93mstopwords\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('stopwords')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mcorpora/stopwords\u001b[0m\n\n  Searched in:\n    - 'C:\\\\Users\\\\abcd/nltk_data'\n    - 'c:\\\\Users\\\\abcd\\\\AppData\\\\Local\\\\Programs\\\\Python\\\\Python311\\\\nltk_data'\n    - 'c:\\\\Users\\\\abcd\\\\AppData\\\\Local\\\\Programs\\\\Python\\\\Python311\\\\share\\\\nltk_data'\n    - 'c:\\\\Users\\\\abcd\\\\AppData\\\\Local\\\\Programs\\\\Python\\\\Python311\\\\lib\\\\nltk_data'\n    - 'C:\\\\Users\\\\abcd\\\\AppData\\\\Roaming\\\\nltk_data'\n    - 'C:\\\\nltk_data'\n    - 'D:\\\\nltk_data'\n    - 'E:\\\\nltk_data'\n**********************************************************************\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mLookupError\u001b[0m                               Traceback (most recent call last)",
      "File \u001b[1;32mc:\\Users\\abcd\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\nltk\\corpus\\util.py:84\u001b[0m, in \u001b[0;36mLazyCorpusLoader.__load\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     83\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m---> 84\u001b[0m     root \u001b[39m=\u001b[39m nltk\u001b[39m.\u001b[39;49mdata\u001b[39m.\u001b[39;49mfind(\u001b[39mf\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39m{\u001b[39;49;00m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msubdir\u001b[39m}\u001b[39;49;00m\u001b[39m/\u001b[39;49m\u001b[39m{\u001b[39;49;00mzip_name\u001b[39m}\u001b[39;49;00m\u001b[39m\"\u001b[39;49m)\n\u001b[0;32m     85\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mLookupError\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\abcd\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\nltk\\data.py:583\u001b[0m, in \u001b[0;36mfind\u001b[1;34m(resource_name, paths)\u001b[0m\n\u001b[0;32m    582\u001b[0m resource_not_found \u001b[39m=\u001b[39m \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m{\u001b[39;00msep\u001b[39m}\u001b[39;00m\u001b[39m\\n\u001b[39;00m\u001b[39m{\u001b[39;00mmsg\u001b[39m}\u001b[39;00m\u001b[39m\\n\u001b[39;00m\u001b[39m{\u001b[39;00msep\u001b[39m}\u001b[39;00m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[1;32m--> 583\u001b[0m \u001b[39mraise\u001b[39;00m \u001b[39mLookupError\u001b[39;00m(resource_not_found)\n",
      "\u001b[1;31mLookupError\u001b[0m: \n**********************************************************************\n  Resource \u001b[93mstopwords\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('stopwords')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mcorpora/stopwords.zip/stopwords/\u001b[0m\n\n  Searched in:\n    - 'C:\\\\Users\\\\abcd/nltk_data'\n    - 'c:\\\\Users\\\\abcd\\\\AppData\\\\Local\\\\Programs\\\\Python\\\\Python311\\\\nltk_data'\n    - 'c:\\\\Users\\\\abcd\\\\AppData\\\\Local\\\\Programs\\\\Python\\\\Python311\\\\share\\\\nltk_data'\n    - 'c:\\\\Users\\\\abcd\\\\AppData\\\\Local\\\\Programs\\\\Python\\\\Python311\\\\lib\\\\nltk_data'\n    - 'C:\\\\Users\\\\abcd\\\\AppData\\\\Roaming\\\\nltk_data'\n    - 'C:\\\\nltk_data'\n    - 'D:\\\\nltk_data'\n    - 'E:\\\\nltk_data'\n**********************************************************************\n",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mLookupError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[17], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m df_data[\u001b[39m'\u001b[39m\u001b[39mCleaned_Speech\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m=\u001b[39m df_data[\u001b[39m'\u001b[39;49m\u001b[39mSpeech\u001b[39;49m\u001b[39m'\u001b[39;49m]\u001b[39m.\u001b[39;49mapply(cleaned)\n",
      "File \u001b[1;32mc:\\Users\\abcd\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pandas\\core\\series.py:4631\u001b[0m, in \u001b[0;36mSeries.apply\u001b[1;34m(self, func, convert_dtype, args, **kwargs)\u001b[0m\n\u001b[0;32m   4521\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mapply\u001b[39m(\n\u001b[0;32m   4522\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[0;32m   4523\u001b[0m     func: AggFuncType,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   4526\u001b[0m     \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs,\n\u001b[0;32m   4527\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m DataFrame \u001b[39m|\u001b[39m Series:\n\u001b[0;32m   4528\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m   4529\u001b[0m \u001b[39m    Invoke function on values of Series.\u001b[39;00m\n\u001b[0;32m   4530\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   4629\u001b[0m \u001b[39m    dtype: float64\u001b[39;00m\n\u001b[0;32m   4630\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m-> 4631\u001b[0m     \u001b[39mreturn\u001b[39;00m SeriesApply(\u001b[39mself\u001b[39;49m, func, convert_dtype, args, kwargs)\u001b[39m.\u001b[39;49mapply()\n",
      "File \u001b[1;32mc:\\Users\\abcd\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pandas\\core\\apply.py:1025\u001b[0m, in \u001b[0;36mSeriesApply.apply\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1022\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mapply_str()\n\u001b[0;32m   1024\u001b[0m \u001b[39m# self.f is Callable\u001b[39;00m\n\u001b[1;32m-> 1025\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mapply_standard()\n",
      "File \u001b[1;32mc:\\Users\\abcd\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pandas\\core\\apply.py:1076\u001b[0m, in \u001b[0;36mSeriesApply.apply_standard\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1074\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m   1075\u001b[0m         values \u001b[39m=\u001b[39m obj\u001b[39m.\u001b[39mastype(\u001b[39mobject\u001b[39m)\u001b[39m.\u001b[39m_values\n\u001b[1;32m-> 1076\u001b[0m         mapped \u001b[39m=\u001b[39m lib\u001b[39m.\u001b[39;49mmap_infer(\n\u001b[0;32m   1077\u001b[0m             values,\n\u001b[0;32m   1078\u001b[0m             f,\n\u001b[0;32m   1079\u001b[0m             convert\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mconvert_dtype,\n\u001b[0;32m   1080\u001b[0m         )\n\u001b[0;32m   1082\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(mapped) \u001b[39mand\u001b[39;00m \u001b[39misinstance\u001b[39m(mapped[\u001b[39m0\u001b[39m], ABCSeries):\n\u001b[0;32m   1083\u001b[0m     \u001b[39m# GH#43986 Need to do list(mapped) in order to get treated as nested\u001b[39;00m\n\u001b[0;32m   1084\u001b[0m     \u001b[39m#  See also GH#25959 regarding EA support\u001b[39;00m\n\u001b[0;32m   1085\u001b[0m     \u001b[39mreturn\u001b[39;00m obj\u001b[39m.\u001b[39m_constructor_expanddim(\u001b[39mlist\u001b[39m(mapped), index\u001b[39m=\u001b[39mobj\u001b[39m.\u001b[39mindex)\n",
      "File \u001b[1;32mc:\\Users\\abcd\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pandas\\_libs\\lib.pyx:2834\u001b[0m, in \u001b[0;36mpandas._libs.lib.map_infer\u001b[1;34m()\u001b[0m\n",
      "Cell \u001b[1;32mIn[12], line 19\u001b[0m, in \u001b[0;36mcleaned\u001b[1;34m(speech)\u001b[0m\n\u001b[0;32m     17\u001b[0m \u001b[39m# Lemmatize each word in the sentence and remove stop words\u001b[39;00m\n\u001b[0;32m     18\u001b[0m lemmatizer \u001b[39m=\u001b[39m WordNetLemmatizer()\n\u001b[1;32m---> 19\u001b[0m review \u001b[39m=\u001b[39m [lemmatizer\u001b[39m.\u001b[39;49mlemmatize(word) \u001b[39mfor\u001b[39;49;00m word \u001b[39min\u001b[39;49;00m review \u001b[39mif\u001b[39;49;00m word \u001b[39mnot\u001b[39;49;00m \u001b[39min\u001b[39;49;00m \u001b[39mset\u001b[39;49m(stopwords\u001b[39m.\u001b[39;49mwords(\u001b[39m\"\u001b[39;49m\u001b[39menglish\u001b[39;49m\u001b[39m\"\u001b[39;49m))]\n\u001b[0;32m     21\u001b[0m \u001b[39m# Convert the list of words back into a sentence\u001b[39;00m\n\u001b[0;32m     22\u001b[0m review \u001b[39m=\u001b[39m \u001b[39m'\u001b[39m\u001b[39m \u001b[39m\u001b[39m'\u001b[39m\u001b[39m.\u001b[39mjoin(review)\n",
      "Cell \u001b[1;32mIn[12], line 19\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     17\u001b[0m \u001b[39m# Lemmatize each word in the sentence and remove stop words\u001b[39;00m\n\u001b[0;32m     18\u001b[0m lemmatizer \u001b[39m=\u001b[39m WordNetLemmatizer()\n\u001b[1;32m---> 19\u001b[0m review \u001b[39m=\u001b[39m [lemmatizer\u001b[39m.\u001b[39mlemmatize(word) \u001b[39mfor\u001b[39;00m word \u001b[39min\u001b[39;00m review \u001b[39mif\u001b[39;00m word \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m \u001b[39mset\u001b[39m(stopwords\u001b[39m.\u001b[39;49mwords(\u001b[39m\"\u001b[39m\u001b[39menglish\u001b[39m\u001b[39m\"\u001b[39m))]\n\u001b[0;32m     21\u001b[0m \u001b[39m# Convert the list of words back into a sentence\u001b[39;00m\n\u001b[0;32m     22\u001b[0m review \u001b[39m=\u001b[39m \u001b[39m'\u001b[39m\u001b[39m \u001b[39m\u001b[39m'\u001b[39m\u001b[39m.\u001b[39mjoin(review)\n",
      "File \u001b[1;32mc:\\Users\\abcd\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\nltk\\corpus\\util.py:121\u001b[0m, in \u001b[0;36mLazyCorpusLoader.__getattr__\u001b[1;34m(self, attr)\u001b[0m\n\u001b[0;32m    118\u001b[0m \u001b[39mif\u001b[39;00m attr \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m__bases__\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[0;32m    119\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mAttributeError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mLazyCorpusLoader object has no attribute \u001b[39m\u001b[39m'\u001b[39m\u001b[39m__bases__\u001b[39m\u001b[39m'\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m--> 121\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m__load()\n\u001b[0;32m    122\u001b[0m \u001b[39m# This looks circular, but its not, since __load() changes our\u001b[39;00m\n\u001b[0;32m    123\u001b[0m \u001b[39m# __class__ to something new:\u001b[39;00m\n\u001b[0;32m    124\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mgetattr\u001b[39m(\u001b[39mself\u001b[39m, attr)\n",
      "File \u001b[1;32mc:\\Users\\abcd\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\nltk\\corpus\\util.py:86\u001b[0m, in \u001b[0;36mLazyCorpusLoader.__load\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     84\u001b[0m             root \u001b[39m=\u001b[39m nltk\u001b[39m.\u001b[39mdata\u001b[39m.\u001b[39mfind(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39msubdir\u001b[39m}\u001b[39;00m\u001b[39m/\u001b[39m\u001b[39m{\u001b[39;00mzip_name\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n\u001b[0;32m     85\u001b[0m         \u001b[39mexcept\u001b[39;00m \u001b[39mLookupError\u001b[39;00m:\n\u001b[1;32m---> 86\u001b[0m             \u001b[39mraise\u001b[39;00m e\n\u001b[0;32m     88\u001b[0m \u001b[39m# Load the corpus.\u001b[39;00m\n\u001b[0;32m     89\u001b[0m corpus \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m__reader_cls(root, \u001b[39m*\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m__args, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m__kwargs)\n",
      "File \u001b[1;32mc:\\Users\\abcd\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\nltk\\corpus\\util.py:81\u001b[0m, in \u001b[0;36mLazyCorpusLoader.__load\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     79\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m     80\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m---> 81\u001b[0m         root \u001b[39m=\u001b[39m nltk\u001b[39m.\u001b[39;49mdata\u001b[39m.\u001b[39;49mfind(\u001b[39mf\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39m{\u001b[39;49;00m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msubdir\u001b[39m}\u001b[39;49;00m\u001b[39m/\u001b[39;49m\u001b[39m{\u001b[39;49;00m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m__name\u001b[39m}\u001b[39;49;00m\u001b[39m\"\u001b[39;49m)\n\u001b[0;32m     82\u001b[0m     \u001b[39mexcept\u001b[39;00m \u001b[39mLookupError\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m     83\u001b[0m         \u001b[39mtry\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\abcd\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\nltk\\data.py:583\u001b[0m, in \u001b[0;36mfind\u001b[1;34m(resource_name, paths)\u001b[0m\n\u001b[0;32m    581\u001b[0m sep \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m*\u001b[39m\u001b[39m\"\u001b[39m \u001b[39m*\u001b[39m \u001b[39m70\u001b[39m\n\u001b[0;32m    582\u001b[0m resource_not_found \u001b[39m=\u001b[39m \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m{\u001b[39;00msep\u001b[39m}\u001b[39;00m\u001b[39m\\n\u001b[39;00m\u001b[39m{\u001b[39;00mmsg\u001b[39m}\u001b[39;00m\u001b[39m\\n\u001b[39;00m\u001b[39m{\u001b[39;00msep\u001b[39m}\u001b[39;00m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[1;32m--> 583\u001b[0m \u001b[39mraise\u001b[39;00m \u001b[39mLookupError\u001b[39;00m(resource_not_found)\n",
      "\u001b[1;31mLookupError\u001b[0m: \n**********************************************************************\n  Resource \u001b[93mstopwords\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('stopwords')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mcorpora/stopwords\u001b[0m\n\n  Searched in:\n    - 'C:\\\\Users\\\\abcd/nltk_data'\n    - 'c:\\\\Users\\\\abcd\\\\AppData\\\\Local\\\\Programs\\\\Python\\\\Python311\\\\nltk_data'\n    - 'c:\\\\Users\\\\abcd\\\\AppData\\\\Local\\\\Programs\\\\Python\\\\Python311\\\\share\\\\nltk_data'\n    - 'c:\\\\Users\\\\abcd\\\\AppData\\\\Local\\\\Programs\\\\Python\\\\Python311\\\\lib\\\\nltk_data'\n    - 'C:\\\\Users\\\\abcd\\\\AppData\\\\Roaming\\\\nltk_data'\n    - 'C:\\\\nltk_data'\n    - 'D:\\\\nltk_data'\n    - 'E:\\\\nltk_data'\n**********************************************************************\n"
     ]
    }
   ],
   "source": [
    "df_data['Cleaned_Speech'] = df_data['Speech'].apply(cleaned)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
