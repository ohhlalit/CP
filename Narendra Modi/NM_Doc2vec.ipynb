{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: gensim in c:\\users\\yatika arora\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (4.3.1)\n",
      "Requirement already satisfied: numpy>=1.18.5 in c:\\users\\yatika arora\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from gensim) (1.24.2)\n",
      "Requirement already satisfied: scipy>=1.7.0 in c:\\users\\yatika arora\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from gensim) (1.10.1)\n",
      "Requirement already satisfied: smart-open>=1.8.1 in c:\\users\\yatika arora\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from gensim) (6.3.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 23.0.1 -> 23.1.2\n",
      "[notice] To update, run: C:\\Users\\Yatika Arora\\AppData\\Local\\Microsoft\\WindowsApps\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "%pip install gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import nltk \n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import gensim\n",
    "from gensim.models import Doc2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"D:\\FINAL_PM_Modi_Speech_Cleaned_string.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>Date</th>\n",
       "      <th>Speech</th>\n",
       "      <th>Title</th>\n",
       "      <th>Cleaned_Speech</th>\n",
       "      <th>Cleaned_Speech_string</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>NMS1</td>\n",
       "      <td>2017-01-03</td>\n",
       "      <td>Governor of Andhra Pradesh, Shri E. S. L. Nara...</td>\n",
       "      <td>PM's Address at the Inauguration of the 104th ...</td>\n",
       "      <td>['governor andhra pradesh shri e l narasimhan ...</td>\n",
       "      <td>governor andhra pradesh shri e l narasimhan ch...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>NMS2</td>\n",
       "      <td>2017-01-09</td>\n",
       "      <td>Chief Minister of Gujarat Shri Vijay Rupani ji...</td>\n",
       "      <td>Text of PM's address at the Inauguration of No...</td>\n",
       "      <td>['chief minister gujarat shri vijay rupani ji ...</td>\n",
       "      <td>chief minister gujarat shri vijay rupani ji co...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>NMS3</td>\n",
       "      <td>2017-01-09</td>\n",
       "      <td>I  am delighted to be here at Gift City to ina...</td>\n",
       "      <td>Text of PM’s address on the occasion of Inaugu...</td>\n",
       "      <td>['delighted gift city inaugurate india first i...</td>\n",
       "      <td>delighted gift city inaugurate india first int...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>NMS4</td>\n",
       "      <td>2017-02-01</td>\n",
       "      <td>I congratulate the Finance Minister Arun Jaitl...</td>\n",
       "      <td>English rendering of the Prime Minister’s Stat...</td>\n",
       "      <td>['congratulate finance minister arun jaitley j...</td>\n",
       "      <td>congratulate finance minister arun jaitley jee...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>NMS5</td>\n",
       "      <td>2017-02-05</td>\n",
       "      <td>Sri Pejavar Math’s most respected Sri Vishwesh...</td>\n",
       "      <td>English rendering of the text of PM’s address ...</td>\n",
       "      <td>['sri pejavar math respected sri vishwesh tirt...</td>\n",
       "      <td>sri pejavar math respected sri vishwesh tirth ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  index        Date                                             Speech   \n",
       "0  NMS1  2017-01-03  Governor of Andhra Pradesh, Shri E. S. L. Nara...  \\\n",
       "1  NMS2  2017-01-09  Chief Minister of Gujarat Shri Vijay Rupani ji...   \n",
       "2  NMS3  2017-01-09  I  am delighted to be here at Gift City to ina...   \n",
       "3  NMS4  2017-02-01  I congratulate the Finance Minister Arun Jaitl...   \n",
       "4  NMS5  2017-02-05  Sri Pejavar Math’s most respected Sri Vishwesh...   \n",
       "\n",
       "                                               Title   \n",
       "0  PM's Address at the Inauguration of the 104th ...  \\\n",
       "1  Text of PM's address at the Inauguration of No...   \n",
       "2  Text of PM’s address on the occasion of Inaugu...   \n",
       "3  English rendering of the Prime Minister’s Stat...   \n",
       "4  English rendering of the text of PM’s address ...   \n",
       "\n",
       "                                      Cleaned_Speech   \n",
       "0  ['governor andhra pradesh shri e l narasimhan ...  \\\n",
       "1  ['chief minister gujarat shri vijay rupani ji ...   \n",
       "2  ['delighted gift city inaugurate india first i...   \n",
       "3  ['congratulate finance minister arun jaitley j...   \n",
       "4  ['sri pejavar math respected sri vishwesh tirt...   \n",
       "\n",
       "                               Cleaned_Speech_string  \n",
       "0  governor andhra pradesh shri e l narasimhan ch...  \n",
       "1  chief minister gujarat shri vijay rupani ji co...  \n",
       "2  delighted gift city inaugurate india first int...  \n",
       "3  congratulate finance minister arun jaitley jee...  \n",
       "4  sri pejavar math respected sri vishwesh tirth ...  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = gensim.models.doc2vec.Doc2Vec(vector_size=50, min_count=2, epochs=40)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_Modi = []\n",
    "for i in df['Cleaned_Speech']:\n",
    "    corpus_Modi += i.replace('[','').replace(']','').replace(\"'\",\"\").split(', ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "#required input was list of list of words of a sentance\n",
    "corpus_tokenized = [nltk.word_tokenize(sentence) for sentence in corpus_Modi]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['governor',\n",
       "  'andhra',\n",
       "  'pradesh',\n",
       "  'shri',\n",
       "  'e',\n",
       "  'l',\n",
       "  'narasimhan',\n",
       "  'chief',\n",
       "  'minister',\n",
       "  'andhra',\n",
       "  'pradesh',\n",
       "  'shri',\n",
       "  'n',\n",
       "  'chandrababu',\n",
       "  'naiduunion',\n",
       "  'minister',\n",
       "  'science',\n",
       "  'technology',\n",
       "  'earth',\n",
       "  'science',\n",
       "  'dr',\n",
       "  'harsh',\n",
       "  'vardhanunion',\n",
       "  'minister',\n",
       "  'state',\n",
       "  'science',\n",
       "  'technology',\n",
       "  'earth',\n",
       "  'science',\n",
       "  'shri',\n",
       "  'chowdarygeneral',\n",
       "  'president',\n",
       "  'indian',\n",
       "  'science',\n",
       "  'congress',\n",
       "  'association',\n",
       "  'professor',\n",
       "  'narayana',\n",
       "  'raovice',\n",
       "  'chancellor',\n",
       "  'sri',\n",
       "  'venkateswara',\n",
       "  'university',\n",
       "  'professor',\n",
       "  'damodaramdistinguished',\n",
       "  'delegatesladies',\n",
       "  'gentleman'],\n",
       " ['delighted',\n",
       "  'begin',\n",
       "  'new',\n",
       "  'year',\n",
       "  'distinguished',\n",
       "  'scientist',\n",
       "  'home',\n",
       "  'abroad',\n",
       "  'holy',\n",
       "  'city',\n",
       "  'tirupati',\n",
       "  'happy',\n",
       "  'inaugurate',\n",
       "  '104th',\n",
       "  'session',\n",
       "  'indian',\n",
       "  'science',\n",
       "  'congress',\n",
       "  'panoramic',\n",
       "  'campus',\n",
       "  'sri',\n",
       "  'venkateswara',\n",
       "  'university',\n",
       "  'appreciate',\n",
       "  'indian',\n",
       "  'science',\n",
       "  'congress',\n",
       "  'association',\n",
       "  'choosing',\n",
       "  'appropriate',\n",
       "  'theme',\n",
       "  'science',\n",
       "  'technology',\n",
       "  'national',\n",
       "  'development',\n",
       "  'year',\n",
       "  'session'],\n",
       " ['distinguished',\n",
       "  'delegate',\n",
       "  'nation',\n",
       "  'always',\n",
       "  'grateful',\n",
       "  'scientist',\n",
       "  'worked',\n",
       "  'tirelessly',\n",
       "  'empower',\n",
       "  'society',\n",
       "  'vision',\n",
       "  'labour',\n",
       "  'leadership'],\n",
       " ['november',\n",
       "  '2016',\n",
       "  'country',\n",
       "  'lost',\n",
       "  'one',\n",
       "  'eminent',\n",
       "  'scientist',\n",
       "  'institution',\n",
       "  'builder',\n",
       "  'dr',\n",
       "  'g',\n",
       "  'k']]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus_tokenized[0:4]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'list' object has no attribute 'words'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[29], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m model\u001b[39m.\u001b[39;49mbuild_vocab(corpus_tokenized)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\gensim\\models\\doc2vec.py:882\u001b[0m, in \u001b[0;36mDoc2Vec.build_vocab\u001b[1;34m(self, corpus_iterable, corpus_file, update, progress_per, keep_raw_vocab, trim_rule, **kwargs)\u001b[0m\n\u001b[0;32m    841\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mbuild_vocab\u001b[39m(\n\u001b[0;32m    842\u001b[0m         \u001b[39mself\u001b[39m, corpus_iterable\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, corpus_file\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, update\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m, progress_per\u001b[39m=\u001b[39m\u001b[39m10000\u001b[39m,\n\u001b[0;32m    843\u001b[0m         keep_raw_vocab\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m, trim_rule\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs,\n\u001b[0;32m    844\u001b[0m     ):\n\u001b[0;32m    845\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Build vocabulary from a sequence of documents (can be a once-only generator stream).\u001b[39;00m\n\u001b[0;32m    846\u001b[0m \n\u001b[0;32m    847\u001b[0m \u001b[39m    Parameters\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    880\u001b[0m \n\u001b[0;32m    881\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 882\u001b[0m     total_words, corpus_count \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mscan_vocab(\n\u001b[0;32m    883\u001b[0m         corpus_iterable\u001b[39m=\u001b[39;49mcorpus_iterable, corpus_file\u001b[39m=\u001b[39;49mcorpus_file,\n\u001b[0;32m    884\u001b[0m         progress_per\u001b[39m=\u001b[39;49mprogress_per, trim_rule\u001b[39m=\u001b[39;49mtrim_rule,\n\u001b[0;32m    885\u001b[0m     )\n\u001b[0;32m    886\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcorpus_count \u001b[39m=\u001b[39m corpus_count\n\u001b[0;32m    887\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcorpus_total_words \u001b[39m=\u001b[39m total_words\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\gensim\\models\\doc2vec.py:1054\u001b[0m, in \u001b[0;36mDoc2Vec.scan_vocab\u001b[1;34m(self, corpus_iterable, corpus_file, progress_per, trim_rule)\u001b[0m\n\u001b[0;32m   1051\u001b[0m \u001b[39mif\u001b[39;00m corpus_file \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m   1052\u001b[0m     corpus_iterable \u001b[39m=\u001b[39m TaggedLineDocument(corpus_file)\n\u001b[1;32m-> 1054\u001b[0m total_words, corpus_count \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_scan_vocab(corpus_iterable, progress_per, trim_rule)\n\u001b[0;32m   1056\u001b[0m logger\u001b[39m.\u001b[39minfo(\n\u001b[0;32m   1057\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mcollected \u001b[39m\u001b[39m%i\u001b[39;00m\u001b[39m word types and \u001b[39m\u001b[39m%i\u001b[39;00m\u001b[39m unique tags from a corpus of \u001b[39m\u001b[39m%i\u001b[39;00m\u001b[39m examples and \u001b[39m\u001b[39m%i\u001b[39;00m\u001b[39m words\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[0;32m   1058\u001b[0m     \u001b[39mlen\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mraw_vocab), \u001b[39mlen\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdv), corpus_count, total_words,\n\u001b[0;32m   1059\u001b[0m )\n\u001b[0;32m   1061\u001b[0m \u001b[39mreturn\u001b[39;00m total_words, corpus_count\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\gensim\\models\\doc2vec.py:956\u001b[0m, in \u001b[0;36mDoc2Vec._scan_vocab\u001b[1;34m(self, corpus_iterable, progress_per, trim_rule)\u001b[0m\n\u001b[0;32m    954\u001b[0m \u001b[39mfor\u001b[39;00m document_no, document \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(corpus_iterable):\n\u001b[0;32m    955\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m checked_string_types:\n\u001b[1;32m--> 956\u001b[0m         \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(document\u001b[39m.\u001b[39;49mwords, \u001b[39mstr\u001b[39m):\n\u001b[0;32m    957\u001b[0m             logger\u001b[39m.\u001b[39mwarning(\n\u001b[0;32m    958\u001b[0m                 \u001b[39m\"\u001b[39m\u001b[39mEach \u001b[39m\u001b[39m'\u001b[39m\u001b[39mwords\u001b[39m\u001b[39m'\u001b[39m\u001b[39m should be a list of words (usually unicode strings). \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    959\u001b[0m                 \u001b[39m\"\u001b[39m\u001b[39mFirst \u001b[39m\u001b[39m'\u001b[39m\u001b[39mwords\u001b[39m\u001b[39m'\u001b[39m\u001b[39m here is instead plain \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[0;32m    960\u001b[0m                 \u001b[39mtype\u001b[39m(document\u001b[39m.\u001b[39mwords),\n\u001b[0;32m    961\u001b[0m             )\n\u001b[0;32m    962\u001b[0m         checked_string_types \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'list' object has no attribute 'words'"
     ]
    }
   ],
   "source": [
    "model.build_vocab(corpus_tokenized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "you must first build vocabulary before training the model",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[22], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m model\u001b[39m.\u001b[39;49mtrain(corpus_Modi, total_examples\u001b[39m=\u001b[39;49mmodel\u001b[39m.\u001b[39;49mcorpus_count, epochs\u001b[39m=\u001b[39;49mmodel\u001b[39m.\u001b[39;49mepochs)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\gensim\\models\\doc2vec.py:516\u001b[0m, in \u001b[0;36mDoc2Vec.train\u001b[1;34m(self, corpus_iterable, corpus_file, total_examples, total_words, epochs, start_alpha, end_alpha, word_count, queue_factor, report_delay, callbacks, **kwargs)\u001b[0m\n\u001b[0;32m    513\u001b[0m     kwargs[\u001b[39m'\u001b[39m\u001b[39moffsets\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m=\u001b[39m offsets\n\u001b[0;32m    514\u001b[0m     kwargs[\u001b[39m'\u001b[39m\u001b[39mstart_doctags\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m=\u001b[39m start_doctags\n\u001b[1;32m--> 516\u001b[0m \u001b[39msuper\u001b[39;49m(Doc2Vec, \u001b[39mself\u001b[39;49m)\u001b[39m.\u001b[39;49mtrain(\n\u001b[0;32m    517\u001b[0m     corpus_iterable\u001b[39m=\u001b[39;49mcorpus_iterable, corpus_file\u001b[39m=\u001b[39;49mcorpus_file,\n\u001b[0;32m    518\u001b[0m     total_examples\u001b[39m=\u001b[39;49mtotal_examples, total_words\u001b[39m=\u001b[39;49mtotal_words,\n\u001b[0;32m    519\u001b[0m     epochs\u001b[39m=\u001b[39;49mepochs, start_alpha\u001b[39m=\u001b[39;49mstart_alpha, end_alpha\u001b[39m=\u001b[39;49mend_alpha, word_count\u001b[39m=\u001b[39;49mword_count,\n\u001b[0;32m    520\u001b[0m     queue_factor\u001b[39m=\u001b[39;49mqueue_factor, report_delay\u001b[39m=\u001b[39;49mreport_delay, callbacks\u001b[39m=\u001b[39;49mcallbacks, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\gensim\\models\\word2vec.py:1045\u001b[0m, in \u001b[0;36mWord2Vec.train\u001b[1;34m(self, corpus_iterable, corpus_file, total_examples, total_words, epochs, start_alpha, end_alpha, word_count, queue_factor, report_delay, compute_loss, callbacks, **kwargs)\u001b[0m\n\u001b[0;32m   1042\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmin_alpha \u001b[39m=\u001b[39m end_alpha \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmin_alpha\n\u001b[0;32m   1043\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mepochs \u001b[39m=\u001b[39m epochs\n\u001b[1;32m-> 1045\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_check_training_sanity(epochs\u001b[39m=\u001b[39;49mepochs, total_examples\u001b[39m=\u001b[39;49mtotal_examples, total_words\u001b[39m=\u001b[39;49mtotal_words)\n\u001b[0;32m   1046\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_check_corpus_sanity(corpus_iterable\u001b[39m=\u001b[39mcorpus_iterable, corpus_file\u001b[39m=\u001b[39mcorpus_file, passes\u001b[39m=\u001b[39mepochs)\n\u001b[0;32m   1048\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39madd_lifecycle_event(\n\u001b[0;32m   1049\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mtrain\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[0;32m   1050\u001b[0m     msg\u001b[39m=\u001b[39m(\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1054\u001b[0m     ),\n\u001b[0;32m   1055\u001b[0m )\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\gensim\\models\\word2vec.py:1554\u001b[0m, in \u001b[0;36mWord2Vec._check_training_sanity\u001b[1;34m(self, epochs, total_examples, total_words, **kwargs)\u001b[0m\n\u001b[0;32m   1551\u001b[0m     logger\u001b[39m.\u001b[39mwarning(\u001b[39m\"\u001b[39m\u001b[39mEffective \u001b[39m\u001b[39m'\u001b[39m\u001b[39malpha\u001b[39m\u001b[39m'\u001b[39m\u001b[39m higher than previous training cycles\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m   1553\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mwv\u001b[39m.\u001b[39mkey_to_index:  \u001b[39m# should be set by `build_vocab`\u001b[39;00m\n\u001b[1;32m-> 1554\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39myou must first build vocabulary before training the model\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m   1555\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mlen\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mwv\u001b[39m.\u001b[39mvectors):\n\u001b[0;32m   1556\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39myou must initialize vectors before training the model\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "\u001b[1;31mRuntimeError\u001b[0m: you must first build vocabulary before training the model"
     ]
    }
   ],
   "source": [
    "model.train(corpus_Modi, total_examples=model.corpus_count, epochs=model.epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
