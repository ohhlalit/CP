{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the libraries \n",
    "import nltk \n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.corpus import stopwords\n",
    "import re\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import numpy as np \n",
    "import gensim\n",
    "from gensim.models import Word2Vec\n",
    "import pandas as pd "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset \n",
    "df_data = pd.read_csv(\"E:\\Tarang\\Ashoka\\Python\\PYTHON PROJECT\\PM_Modi_Speech_Text_final.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initializing the model \n",
    "lemmatizer = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(speech):\n",
    "    # Sentence Tokenization\n",
    "    sentence = nltk.sent_tokenize(speech)\n",
    "\n",
    "    # Creating an empty list to store the cleaned dataset \n",
    "    corpus = []\n",
    "\n",
    "    # Loop through each sentence in the speech\n",
    "    for i in range(len(sentence)):\n",
    "        # Remove any non-alphabetic characters in the sentence and convert to lowercase\n",
    "        review = re.sub(\"[^a-zA-Z]\", \" \", sentence[i])\n",
    "        review = review.lower()\n",
    "        \n",
    "        # Tokenize the sentence into a list of words\n",
    "        review = review.split()\n",
    "        \n",
    "        # Lemmatize each word in the sentence and remove stop words\n",
    "        lemmatizer = WordNetLemmatizer()\n",
    "        review = [lemmatizer.lemmatize(word) for word in review if word not in set(stopwords.words(\"english\"))]\n",
    "        \n",
    "        # Convert the list of words back into a sentence\n",
    "        review = ' '.join(review)\n",
    "        \n",
    "        # Add the cleaned sentence to the corpus\n",
    "        corpus.append(review)\n",
    "\n",
    "    # Tokenize each sentence in the corpus into a list of words\n",
    "    corpus_tokenized = [nltk.word_tokenize(sentence) for sentence in corpus]\n",
    "\n",
    "    # Train the Word2Vec model on the tokenized corpus\n",
    "    model = Word2Vec(sentences=corpus_tokenized, vector_size=300, window=5, min_count=1, workers=4)\n",
    "\n",
    "    # Extract the word vectors for each word in the corpus\n",
    "    vectors = []\n",
    "    for sentence in corpus_tokenized:\n",
    "        for word in sentence:\n",
    "            if word in model.wv.key_to_index:\n",
    "                # If the word is in the Word2Vec model's vocabulary, extract its vector\n",
    "                word_vector = model.wv.get_vector(word)\n",
    "                vectors.append(word_vector)\n",
    "            else:\n",
    "                # If the word is not in the vocabulary, print a message to the console\n",
    "                print(f\"{word} not in vocabulary.\")\n",
    "    \n",
    "    # Calculate the average vector for the entire speech\n",
    "    vectors_array = np.array(vectors)\n",
    "    avg_vector = np.mean(vectors_array, axis=0)\n",
    "    \n",
    "    return avg_vector\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converts the Speech column to a string data type.\n",
    "df_data['Speech'] = df_data['Speech'].astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "you must first build vocabulary before training the model",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[12], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[39m# Vectorize every speech \u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m df_data[\u001b[39m'\u001b[39m\u001b[39mVectorized_Speech\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m=\u001b[39m df_data[\u001b[39m'\u001b[39;49m\u001b[39mSpeech\u001b[39;49m\u001b[39m'\u001b[39;49m]\u001b[39m.\u001b[39;49mapply(tokenize)\n",
      "File \u001b[1;32mc:\\Users\\hp\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pandas\\core\\series.py:4771\u001b[0m, in \u001b[0;36mSeries.apply\u001b[1;34m(self, func, convert_dtype, args, **kwargs)\u001b[0m\n\u001b[0;32m   4661\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mapply\u001b[39m(\n\u001b[0;32m   4662\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[0;32m   4663\u001b[0m     func: AggFuncType,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   4666\u001b[0m     \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs,\n\u001b[0;32m   4667\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m DataFrame \u001b[39m|\u001b[39m Series:\n\u001b[0;32m   4668\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m   4669\u001b[0m \u001b[39m    Invoke function on values of Series.\u001b[39;00m\n\u001b[0;32m   4670\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   4769\u001b[0m \u001b[39m    dtype: float64\u001b[39;00m\n\u001b[0;32m   4770\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m-> 4771\u001b[0m     \u001b[39mreturn\u001b[39;00m SeriesApply(\u001b[39mself\u001b[39;49m, func, convert_dtype, args, kwargs)\u001b[39m.\u001b[39;49mapply()\n",
      "File \u001b[1;32mc:\\Users\\hp\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pandas\\core\\apply.py:1123\u001b[0m, in \u001b[0;36mSeriesApply.apply\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1120\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mapply_str()\n\u001b[0;32m   1122\u001b[0m \u001b[39m# self.f is Callable\u001b[39;00m\n\u001b[1;32m-> 1123\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mapply_standard()\n",
      "File \u001b[1;32mc:\\Users\\hp\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pandas\\core\\apply.py:1174\u001b[0m, in \u001b[0;36mSeriesApply.apply_standard\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1172\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m   1173\u001b[0m         values \u001b[39m=\u001b[39m obj\u001b[39m.\u001b[39mastype(\u001b[39mobject\u001b[39m)\u001b[39m.\u001b[39m_values\n\u001b[1;32m-> 1174\u001b[0m         mapped \u001b[39m=\u001b[39m lib\u001b[39m.\u001b[39;49mmap_infer(\n\u001b[0;32m   1175\u001b[0m             values,\n\u001b[0;32m   1176\u001b[0m             f,\n\u001b[0;32m   1177\u001b[0m             convert\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mconvert_dtype,\n\u001b[0;32m   1178\u001b[0m         )\n\u001b[0;32m   1180\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(mapped) \u001b[39mand\u001b[39;00m \u001b[39misinstance\u001b[39m(mapped[\u001b[39m0\u001b[39m], ABCSeries):\n\u001b[0;32m   1181\u001b[0m     \u001b[39m# GH#43986 Need to do list(mapped) in order to get treated as nested\u001b[39;00m\n\u001b[0;32m   1182\u001b[0m     \u001b[39m#  See also GH#25959 regarding EA support\u001b[39;00m\n\u001b[0;32m   1183\u001b[0m     \u001b[39mreturn\u001b[39;00m obj\u001b[39m.\u001b[39m_constructor_expanddim(\u001b[39mlist\u001b[39m(mapped), index\u001b[39m=\u001b[39mobj\u001b[39m.\u001b[39mindex)\n",
      "File \u001b[1;32mc:\\Users\\hp\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pandas\\_libs\\lib.pyx:2924\u001b[0m, in \u001b[0;36mpandas._libs.lib.map_infer\u001b[1;34m()\u001b[0m\n",
      "Cell \u001b[1;32mIn[4], line 31\u001b[0m, in \u001b[0;36mtokenize\u001b[1;34m(speech)\u001b[0m\n\u001b[0;32m     28\u001b[0m corpus_tokenized \u001b[39m=\u001b[39m [nltk\u001b[39m.\u001b[39mword_tokenize(sentence) \u001b[39mfor\u001b[39;00m sentence \u001b[39min\u001b[39;00m corpus]\n\u001b[0;32m     30\u001b[0m \u001b[39m# Train the Word2Vec model on the tokenized corpus\u001b[39;00m\n\u001b[1;32m---> 31\u001b[0m model \u001b[39m=\u001b[39m Word2Vec(sentences\u001b[39m=\u001b[39;49mcorpus_tokenized, vector_size\u001b[39m=\u001b[39;49m\u001b[39m300\u001b[39;49m, window\u001b[39m=\u001b[39;49m\u001b[39m5\u001b[39;49m, min_count\u001b[39m=\u001b[39;49m\u001b[39m1\u001b[39;49m, workers\u001b[39m=\u001b[39;49m\u001b[39m4\u001b[39;49m)\n\u001b[0;32m     33\u001b[0m \u001b[39m# Extract the word vectors for each word in the corpus\u001b[39;00m\n\u001b[0;32m     34\u001b[0m vectors \u001b[39m=\u001b[39m []\n",
      "File \u001b[1;32mc:\\Users\\hp\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\gensim\\models\\word2vec.py:430\u001b[0m, in \u001b[0;36mWord2Vec.__init__\u001b[1;34m(self, sentences, corpus_file, vector_size, alpha, window, min_count, max_vocab_size, sample, seed, workers, min_alpha, sg, hs, negative, ns_exponent, cbow_mean, hashfxn, epochs, null_word, trim_rule, sorted_vocab, batch_words, compute_loss, callbacks, comment, max_final_vocab, shrink_windows)\u001b[0m\n\u001b[0;32m    428\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_check_corpus_sanity(corpus_iterable\u001b[39m=\u001b[39mcorpus_iterable, corpus_file\u001b[39m=\u001b[39mcorpus_file, passes\u001b[39m=\u001b[39m(epochs \u001b[39m+\u001b[39m \u001b[39m1\u001b[39m))\n\u001b[0;32m    429\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbuild_vocab(corpus_iterable\u001b[39m=\u001b[39mcorpus_iterable, corpus_file\u001b[39m=\u001b[39mcorpus_file, trim_rule\u001b[39m=\u001b[39mtrim_rule)\n\u001b[1;32m--> 430\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtrain(\n\u001b[0;32m    431\u001b[0m         corpus_iterable\u001b[39m=\u001b[39;49mcorpus_iterable, corpus_file\u001b[39m=\u001b[39;49mcorpus_file, total_examples\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcorpus_count,\n\u001b[0;32m    432\u001b[0m         total_words\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcorpus_total_words, epochs\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mepochs, start_alpha\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49malpha,\n\u001b[0;32m    433\u001b[0m         end_alpha\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmin_alpha, compute_loss\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcompute_loss, callbacks\u001b[39m=\u001b[39;49mcallbacks)\n\u001b[0;32m    434\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    435\u001b[0m     \u001b[39mif\u001b[39;00m trim_rule \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\hp\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\gensim\\models\\word2vec.py:1045\u001b[0m, in \u001b[0;36mWord2Vec.train\u001b[1;34m(self, corpus_iterable, corpus_file, total_examples, total_words, epochs, start_alpha, end_alpha, word_count, queue_factor, report_delay, compute_loss, callbacks, **kwargs)\u001b[0m\n\u001b[0;32m   1042\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmin_alpha \u001b[39m=\u001b[39m end_alpha \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmin_alpha\n\u001b[0;32m   1043\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mepochs \u001b[39m=\u001b[39m epochs\n\u001b[1;32m-> 1045\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_check_training_sanity(epochs\u001b[39m=\u001b[39;49mepochs, total_examples\u001b[39m=\u001b[39;49mtotal_examples, total_words\u001b[39m=\u001b[39;49mtotal_words)\n\u001b[0;32m   1046\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_check_corpus_sanity(corpus_iterable\u001b[39m=\u001b[39mcorpus_iterable, corpus_file\u001b[39m=\u001b[39mcorpus_file, passes\u001b[39m=\u001b[39mepochs)\n\u001b[0;32m   1048\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39madd_lifecycle_event(\n\u001b[0;32m   1049\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mtrain\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[0;32m   1050\u001b[0m     msg\u001b[39m=\u001b[39m(\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1054\u001b[0m     ),\n\u001b[0;32m   1055\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\hp\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\gensim\\models\\word2vec.py:1554\u001b[0m, in \u001b[0;36mWord2Vec._check_training_sanity\u001b[1;34m(self, epochs, total_examples, total_words, **kwargs)\u001b[0m\n\u001b[0;32m   1551\u001b[0m     logger\u001b[39m.\u001b[39mwarning(\u001b[39m\"\u001b[39m\u001b[39mEffective \u001b[39m\u001b[39m'\u001b[39m\u001b[39malpha\u001b[39m\u001b[39m'\u001b[39m\u001b[39m higher than previous training cycles\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m   1553\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mwv\u001b[39m.\u001b[39mkey_to_index:  \u001b[39m# should be set by `build_vocab`\u001b[39;00m\n\u001b[1;32m-> 1554\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39myou must first build vocabulary before training the model\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m   1555\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mlen\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mwv\u001b[39m.\u001b[39mvectors):\n\u001b[0;32m   1556\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39myou must initialize vectors before training the model\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "\u001b[1;31mRuntimeError\u001b[0m: you must first build vocabulary before training the model"
     ]
    }
   ],
   "source": [
    "# Vectorize every speech \n",
    "df_data['Vectorized_Speech'] = df_data['Speech'].apply(tokenize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(speech):\n",
    "    # Sentence Tokenization\n",
    "    sentence = nltk.sent_tokenize(speech)\n",
    "\n",
    "    # Creating an empty list to store the cleaned dataset \n",
    "    corpus = []\n",
    "\n",
    "    # Loop through each sentence in the speech\n",
    "    for i in range(len(sentence)):\n",
    "        # Remove any non-alphabetic characters in the sentence and convert to lowercase\n",
    "        review = re.sub(\"[^a-zA-Z]\", \" \", sentence[i])\n",
    "        review = review.lower()\n",
    "        \n",
    "        # Tokenize the sentence into a list of words\n",
    "        review = review.split()\n",
    "        \n",
    "        # Lemmatize each word in the sentence and remove stop words\n",
    "        lemmatizer = WordNetLemmatizer()\n",
    "        review = [lemmatizer.lemmatize(word) for word in review if word not in set(stopwords.words(\"english\"))]\n",
    "        \n",
    "        # Convert the list of words back into a sentence\n",
    "        review = ' '.join(review)\n",
    "        \n",
    "        # Add the cleaned sentence to the corpus\n",
    "        corpus.append(review)\n",
    "\n",
    "    # Tokenize each sentence in the corpus into a list of words\n",
    "    corpus_tokenized = [nltk.word_tokenize(sentence) for sentence in corpus]\n",
    "\n",
    "    # Build the vocabulary for the Word2Vec model\n",
    "    model = Word2Vec(vector_size=300, window=5, min_count=1, workers=4)\n",
    "    model.build_vocab(sentences=corpus_tokenized)\n",
    "\n",
    "    # Train the Word2Vec model on the tokenized corpus\n",
    "    #model.train(sentences=corpus_tokenized, total_examples=len(corpus_tokenized), epochs=100)\n",
    "\n",
    "    # Extract the word vectors for each word in the corpus\n",
    "    vectors = []\n",
    "    for sentence in corpus_tokenized:\n",
    "        for word in sentence:\n",
    "            if word in model.wv.key_to_index:\n",
    "                # If the word is in the Word2Vec model's vocabulary, extract its vector\n",
    "                word_vector = model.wv.get_vector(word)\n",
    "                vectors.append(word_vector)\n",
    "            else:\n",
    "                # If the word is not in the vocabulary, print a message to the console\n",
    "                print(f\"{word} not in vocabulary.\")\n",
    "    \n",
    "    # Calculate the average vector for the entire speech\n",
    "    vectors_array = np.array(vectors)\n",
    "    avg_vector = np.mean(vectors_array, axis=0)\n",
    "    \n",
    "    return avg_vector\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "Either one of corpus_file or corpus_iterable value must be provided",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[20], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[39m# Vectorize every speech \u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m df_data[\u001b[39m'\u001b[39m\u001b[39mVectorized_Speech\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m=\u001b[39m df_data[\u001b[39m'\u001b[39;49m\u001b[39mSpeech\u001b[39;49m\u001b[39m'\u001b[39;49m]\u001b[39m.\u001b[39;49mapply(tokenize)\n",
      "File \u001b[1;32mc:\\Users\\hp\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pandas\\core\\series.py:4771\u001b[0m, in \u001b[0;36mSeries.apply\u001b[1;34m(self, func, convert_dtype, args, **kwargs)\u001b[0m\n\u001b[0;32m   4661\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mapply\u001b[39m(\n\u001b[0;32m   4662\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[0;32m   4663\u001b[0m     func: AggFuncType,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   4666\u001b[0m     \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs,\n\u001b[0;32m   4667\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m DataFrame \u001b[39m|\u001b[39m Series:\n\u001b[0;32m   4668\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m   4669\u001b[0m \u001b[39m    Invoke function on values of Series.\u001b[39;00m\n\u001b[0;32m   4670\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   4769\u001b[0m \u001b[39m    dtype: float64\u001b[39;00m\n\u001b[0;32m   4770\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m-> 4771\u001b[0m     \u001b[39mreturn\u001b[39;00m SeriesApply(\u001b[39mself\u001b[39;49m, func, convert_dtype, args, kwargs)\u001b[39m.\u001b[39;49mapply()\n",
      "File \u001b[1;32mc:\\Users\\hp\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pandas\\core\\apply.py:1123\u001b[0m, in \u001b[0;36mSeriesApply.apply\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1120\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mapply_str()\n\u001b[0;32m   1122\u001b[0m \u001b[39m# self.f is Callable\u001b[39;00m\n\u001b[1;32m-> 1123\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mapply_standard()\n",
      "File \u001b[1;32mc:\\Users\\hp\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pandas\\core\\apply.py:1174\u001b[0m, in \u001b[0;36mSeriesApply.apply_standard\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1172\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m   1173\u001b[0m         values \u001b[39m=\u001b[39m obj\u001b[39m.\u001b[39mastype(\u001b[39mobject\u001b[39m)\u001b[39m.\u001b[39m_values\n\u001b[1;32m-> 1174\u001b[0m         mapped \u001b[39m=\u001b[39m lib\u001b[39m.\u001b[39;49mmap_infer(\n\u001b[0;32m   1175\u001b[0m             values,\n\u001b[0;32m   1176\u001b[0m             f,\n\u001b[0;32m   1177\u001b[0m             convert\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mconvert_dtype,\n\u001b[0;32m   1178\u001b[0m         )\n\u001b[0;32m   1180\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(mapped) \u001b[39mand\u001b[39;00m \u001b[39misinstance\u001b[39m(mapped[\u001b[39m0\u001b[39m], ABCSeries):\n\u001b[0;32m   1181\u001b[0m     \u001b[39m# GH#43986 Need to do list(mapped) in order to get treated as nested\u001b[39;00m\n\u001b[0;32m   1182\u001b[0m     \u001b[39m#  See also GH#25959 regarding EA support\u001b[39;00m\n\u001b[0;32m   1183\u001b[0m     \u001b[39mreturn\u001b[39;00m obj\u001b[39m.\u001b[39m_constructor_expanddim(\u001b[39mlist\u001b[39m(mapped), index\u001b[39m=\u001b[39mobj\u001b[39m.\u001b[39mindex)\n",
      "File \u001b[1;32mc:\\Users\\hp\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pandas\\_libs\\lib.pyx:2924\u001b[0m, in \u001b[0;36mpandas._libs.lib.map_infer\u001b[1;34m()\u001b[0m\n",
      "Cell \u001b[1;32mIn[17], line 32\u001b[0m, in \u001b[0;36mtokenize\u001b[1;34m(speech)\u001b[0m\n\u001b[0;32m     30\u001b[0m \u001b[39m# Build the vocabulary for the Word2Vec model\u001b[39;00m\n\u001b[0;32m     31\u001b[0m model \u001b[39m=\u001b[39m Word2Vec(vector_size\u001b[39m=\u001b[39m\u001b[39m300\u001b[39m, window\u001b[39m=\u001b[39m\u001b[39m5\u001b[39m, min_count\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m, workers\u001b[39m=\u001b[39m\u001b[39m4\u001b[39m)\n\u001b[1;32m---> 32\u001b[0m model\u001b[39m.\u001b[39;49mbuild_vocab(sentences\u001b[39m=\u001b[39;49mcorpus_tokenized)\n\u001b[0;32m     34\u001b[0m \u001b[39m# Train the Word2Vec model on the tokenized corpus\u001b[39;00m\n\u001b[0;32m     35\u001b[0m \u001b[39m#model.train(sentences=corpus_tokenized, total_examples=len(corpus_tokenized), epochs=100)\u001b[39;00m\n\u001b[0;32m     36\u001b[0m \n\u001b[0;32m     37\u001b[0m \u001b[39m# Extract the word vectors for each word in the corpus\u001b[39;00m\n\u001b[0;32m     38\u001b[0m vectors \u001b[39m=\u001b[39m []\n",
      "File \u001b[1;32mc:\\Users\\hp\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\gensim\\models\\word2vec.py:490\u001b[0m, in \u001b[0;36mWord2Vec.build_vocab\u001b[1;34m(self, corpus_iterable, corpus_file, update, progress_per, keep_raw_vocab, trim_rule, **kwargs)\u001b[0m\n\u001b[0;32m    449\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mbuild_vocab\u001b[39m(\n\u001b[0;32m    450\u001b[0m         \u001b[39mself\u001b[39m, corpus_iterable\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, corpus_file\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, update\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m, progress_per\u001b[39m=\u001b[39m\u001b[39m10000\u001b[39m,\n\u001b[0;32m    451\u001b[0m         keep_raw_vocab\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m, trim_rule\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs,\n\u001b[0;32m    452\u001b[0m     ):\n\u001b[0;32m    453\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Build vocabulary from a sequence of sentences (can be a once-only generator stream).\u001b[39;00m\n\u001b[0;32m    454\u001b[0m \n\u001b[0;32m    455\u001b[0m \u001b[39m    Parameters\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    488\u001b[0m \n\u001b[0;32m    489\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 490\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_check_corpus_sanity(corpus_iterable\u001b[39m=\u001b[39;49mcorpus_iterable, corpus_file\u001b[39m=\u001b[39;49mcorpus_file, passes\u001b[39m=\u001b[39;49m\u001b[39m1\u001b[39;49m)\n\u001b[0;32m    491\u001b[0m     total_words, corpus_count \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mscan_vocab(\n\u001b[0;32m    492\u001b[0m         corpus_iterable\u001b[39m=\u001b[39mcorpus_iterable, corpus_file\u001b[39m=\u001b[39mcorpus_file, progress_per\u001b[39m=\u001b[39mprogress_per, trim_rule\u001b[39m=\u001b[39mtrim_rule)\n\u001b[0;32m    493\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcorpus_count \u001b[39m=\u001b[39m corpus_count\n",
      "File \u001b[1;32mc:\\Users\\hp\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\gensim\\models\\word2vec.py:1497\u001b[0m, in \u001b[0;36mWord2Vec._check_corpus_sanity\u001b[1;34m(self, corpus_iterable, corpus_file, passes)\u001b[0m\n\u001b[0;32m   1495\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"Checks whether the corpus parameters make sense.\"\"\"\u001b[39;00m\n\u001b[0;32m   1496\u001b[0m \u001b[39mif\u001b[39;00m corpus_file \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m corpus_iterable \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m-> 1497\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mTypeError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mEither one of corpus_file or corpus_iterable value must be provided\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m corpus_file \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m corpus_iterable \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m   1499\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mTypeError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mBoth corpus_file and corpus_iterable must not be provided at the same time\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "\u001b[1;31mTypeError\u001b[0m: Either one of corpus_file or corpus_iterable value must be provided"
     ]
    }
   ],
   "source": [
    "\n",
    "# Vectorize every speech \n",
    "df_data['Vectorized_Speech'] = df_data['Speech'].apply(tokenize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\hp\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\hp\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\hp\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "you must first build vocabulary before training the model",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[21], line 62\u001b[0m\n\u001b[0;32m     59\u001b[0m     \u001b[39mreturn\u001b[39;00m avg_vector\n\u001b[0;32m     61\u001b[0m \u001b[39m# Vectorize every speech \u001b[39;00m\n\u001b[1;32m---> 62\u001b[0m df_data[\u001b[39m'\u001b[39m\u001b[39mVectorized_Speech\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m=\u001b[39m df_data[\u001b[39m'\u001b[39;49m\u001b[39mSpeech\u001b[39;49m\u001b[39m'\u001b[39;49m]\u001b[39m.\u001b[39;49mapply(tokenize)\n",
      "File \u001b[1;32mc:\\Users\\hp\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pandas\\core\\series.py:4771\u001b[0m, in \u001b[0;36mSeries.apply\u001b[1;34m(self, func, convert_dtype, args, **kwargs)\u001b[0m\n\u001b[0;32m   4661\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mapply\u001b[39m(\n\u001b[0;32m   4662\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[0;32m   4663\u001b[0m     func: AggFuncType,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   4666\u001b[0m     \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs,\n\u001b[0;32m   4667\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m DataFrame \u001b[39m|\u001b[39m Series:\n\u001b[0;32m   4668\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m   4669\u001b[0m \u001b[39m    Invoke function on values of Series.\u001b[39;00m\n\u001b[0;32m   4670\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   4769\u001b[0m \u001b[39m    dtype: float64\u001b[39;00m\n\u001b[0;32m   4770\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m-> 4771\u001b[0m     \u001b[39mreturn\u001b[39;00m SeriesApply(\u001b[39mself\u001b[39;49m, func, convert_dtype, args, kwargs)\u001b[39m.\u001b[39;49mapply()\n",
      "File \u001b[1;32mc:\\Users\\hp\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pandas\\core\\apply.py:1123\u001b[0m, in \u001b[0;36mSeriesApply.apply\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1120\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mapply_str()\n\u001b[0;32m   1122\u001b[0m \u001b[39m# self.f is Callable\u001b[39;00m\n\u001b[1;32m-> 1123\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mapply_standard()\n",
      "File \u001b[1;32mc:\\Users\\hp\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pandas\\core\\apply.py:1174\u001b[0m, in \u001b[0;36mSeriesApply.apply_standard\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1172\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m   1173\u001b[0m         values \u001b[39m=\u001b[39m obj\u001b[39m.\u001b[39mastype(\u001b[39mobject\u001b[39m)\u001b[39m.\u001b[39m_values\n\u001b[1;32m-> 1174\u001b[0m         mapped \u001b[39m=\u001b[39m lib\u001b[39m.\u001b[39;49mmap_infer(\n\u001b[0;32m   1175\u001b[0m             values,\n\u001b[0;32m   1176\u001b[0m             f,\n\u001b[0;32m   1177\u001b[0m             convert\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mconvert_dtype,\n\u001b[0;32m   1178\u001b[0m         )\n\u001b[0;32m   1180\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(mapped) \u001b[39mand\u001b[39;00m \u001b[39misinstance\u001b[39m(mapped[\u001b[39m0\u001b[39m], ABCSeries):\n\u001b[0;32m   1181\u001b[0m     \u001b[39m# GH#43986 Need to do list(mapped) in order to get treated as nested\u001b[39;00m\n\u001b[0;32m   1182\u001b[0m     \u001b[39m#  See also GH#25959 regarding EA support\u001b[39;00m\n\u001b[0;32m   1183\u001b[0m     \u001b[39mreturn\u001b[39;00m obj\u001b[39m.\u001b[39m_constructor_expanddim(\u001b[39mlist\u001b[39m(mapped), index\u001b[39m=\u001b[39mobj\u001b[39m.\u001b[39mindex)\n",
      "File \u001b[1;32mc:\\Users\\hp\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pandas\\_libs\\lib.pyx:2924\u001b[0m, in \u001b[0;36mpandas._libs.lib.map_infer\u001b[1;34m()\u001b[0m\n",
      "Cell \u001b[1;32mIn[21], line 41\u001b[0m, in \u001b[0;36mtokenize\u001b[1;34m(speech)\u001b[0m\n\u001b[0;32m     38\u001b[0m corpus_tokenized \u001b[39m=\u001b[39m [nltk\u001b[39m.\u001b[39mword_tokenize(sentence) \u001b[39mfor\u001b[39;00m sentence \u001b[39min\u001b[39;00m corpus]\n\u001b[0;32m     40\u001b[0m \u001b[39m# Train the Word2Vec model on the tokenized corpus\u001b[39;00m\n\u001b[1;32m---> 41\u001b[0m model \u001b[39m=\u001b[39m Word2Vec(corpus_tokenized, vector_size\u001b[39m=\u001b[39;49m\u001b[39m300\u001b[39;49m, window\u001b[39m=\u001b[39;49m\u001b[39m5\u001b[39;49m, min_count\u001b[39m=\u001b[39;49m\u001b[39m1\u001b[39;49m, workers\u001b[39m=\u001b[39;49m\u001b[39m4\u001b[39;49m)\n\u001b[0;32m     43\u001b[0m \u001b[39m# Extract the word vectors for each word in the corpus\u001b[39;00m\n\u001b[0;32m     44\u001b[0m vectors \u001b[39m=\u001b[39m []\n",
      "File \u001b[1;32mc:\\Users\\hp\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\gensim\\models\\word2vec.py:430\u001b[0m, in \u001b[0;36mWord2Vec.__init__\u001b[1;34m(self, sentences, corpus_file, vector_size, alpha, window, min_count, max_vocab_size, sample, seed, workers, min_alpha, sg, hs, negative, ns_exponent, cbow_mean, hashfxn, epochs, null_word, trim_rule, sorted_vocab, batch_words, compute_loss, callbacks, comment, max_final_vocab, shrink_windows)\u001b[0m\n\u001b[0;32m    428\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_check_corpus_sanity(corpus_iterable\u001b[39m=\u001b[39mcorpus_iterable, corpus_file\u001b[39m=\u001b[39mcorpus_file, passes\u001b[39m=\u001b[39m(epochs \u001b[39m+\u001b[39m \u001b[39m1\u001b[39m))\n\u001b[0;32m    429\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbuild_vocab(corpus_iterable\u001b[39m=\u001b[39mcorpus_iterable, corpus_file\u001b[39m=\u001b[39mcorpus_file, trim_rule\u001b[39m=\u001b[39mtrim_rule)\n\u001b[1;32m--> 430\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtrain(\n\u001b[0;32m    431\u001b[0m         corpus_iterable\u001b[39m=\u001b[39;49mcorpus_iterable, corpus_file\u001b[39m=\u001b[39;49mcorpus_file, total_examples\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcorpus_count,\n\u001b[0;32m    432\u001b[0m         total_words\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcorpus_total_words, epochs\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mepochs, start_alpha\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49malpha,\n\u001b[0;32m    433\u001b[0m         end_alpha\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmin_alpha, compute_loss\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcompute_loss, callbacks\u001b[39m=\u001b[39;49mcallbacks)\n\u001b[0;32m    434\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    435\u001b[0m     \u001b[39mif\u001b[39;00m trim_rule \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\hp\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\gensim\\models\\word2vec.py:1045\u001b[0m, in \u001b[0;36mWord2Vec.train\u001b[1;34m(self, corpus_iterable, corpus_file, total_examples, total_words, epochs, start_alpha, end_alpha, word_count, queue_factor, report_delay, compute_loss, callbacks, **kwargs)\u001b[0m\n\u001b[0;32m   1042\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmin_alpha \u001b[39m=\u001b[39m end_alpha \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmin_alpha\n\u001b[0;32m   1043\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mepochs \u001b[39m=\u001b[39m epochs\n\u001b[1;32m-> 1045\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_check_training_sanity(epochs\u001b[39m=\u001b[39;49mepochs, total_examples\u001b[39m=\u001b[39;49mtotal_examples, total_words\u001b[39m=\u001b[39;49mtotal_words)\n\u001b[0;32m   1046\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_check_corpus_sanity(corpus_iterable\u001b[39m=\u001b[39mcorpus_iterable, corpus_file\u001b[39m=\u001b[39mcorpus_file, passes\u001b[39m=\u001b[39mepochs)\n\u001b[0;32m   1048\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39madd_lifecycle_event(\n\u001b[0;32m   1049\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mtrain\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[0;32m   1050\u001b[0m     msg\u001b[39m=\u001b[39m(\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1054\u001b[0m     ),\n\u001b[0;32m   1055\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\hp\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\gensim\\models\\word2vec.py:1554\u001b[0m, in \u001b[0;36mWord2Vec._check_training_sanity\u001b[1;34m(self, epochs, total_examples, total_words, **kwargs)\u001b[0m\n\u001b[0;32m   1551\u001b[0m     logger\u001b[39m.\u001b[39mwarning(\u001b[39m\"\u001b[39m\u001b[39mEffective \u001b[39m\u001b[39m'\u001b[39m\u001b[39malpha\u001b[39m\u001b[39m'\u001b[39m\u001b[39m higher than previous training cycles\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m   1553\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mwv\u001b[39m.\u001b[39mkey_to_index:  \u001b[39m# should be set by `build_vocab`\u001b[39;00m\n\u001b[1;32m-> 1554\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39myou must first build vocabulary before training the model\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m   1555\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mlen\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mwv\u001b[39m.\u001b[39mvectors):\n\u001b[0;32m   1556\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39myou must initialize vectors before training the model\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "\u001b[1;31mRuntimeError\u001b[0m: you must first build vocabulary before training the model"
     ]
    }
   ],
   "source": [
    "\n",
    "def tokenize(speech):\n",
    "    # Sentence Tokenization\n",
    "    sentence = nltk.sent_tokenize(speech)\n",
    "\n",
    "    # Creating an empty list to store the cleaned dataset \n",
    "    corpus = []\n",
    "\n",
    "    # Loop through each sentence in the speech\n",
    "    for i in range(len(sentence)):\n",
    "        # Remove any non-alphabetic characters in the sentence and convert to lowercase\n",
    "        review = re.sub(\"[^a-zA-Z]\", \" \", sentence[i])\n",
    "        review = review.lower()\n",
    "        \n",
    "        # Tokenize the sentence into a list of words\n",
    "        review = review.split()\n",
    "        \n",
    "        # Lemmatize each word in the sentence and remove stop words\n",
    "        lemmatizer = WordNetLemmatizer()\n",
    "        review = [lemmatizer.lemmatize(word) for word in review if word not in set(stopwords.words(\"english\"))]\n",
    "        \n",
    "        # Convert the list of words back into a sentence\n",
    "        review = ' '.join(review)\n",
    "        \n",
    "        # Add the cleaned sentence to the corpus\n",
    "        corpus.append(review)\n",
    "\n",
    "    # Tokenize each sentence in the corpus into a list of words\n",
    "    corpus_tokenized = [nltk.word_tokenize(sentence) for sentence in corpus]\n",
    "\n",
    "    # Train the Word2Vec model on the tokenized corpus\n",
    "    model = Word2Vec(corpus_tokenized, vector_size=300, window=5, min_count=1, workers=4)\n",
    "\n",
    "    # Extract the word vectors for each word in the corpus\n",
    "    vectors = []\n",
    "    for sentence in corpus_tokenized:\n",
    "        for word in sentence:\n",
    "            if word in model.wv.key_to_index:\n",
    "                # If the word is in the Word2Vec model's vocabulary, extract its vector\n",
    "                word_vector = model.wv.get_vector(word)\n",
    "                vectors.append(word_vector)\n",
    "            else:\n",
    "                # If the word is not in the vocabulary, print a message to the console\n",
    "                print(f\"{word} not in vocabulary.\")\n",
    "    \n",
    "    # Calculate the average vector for the entire speech\n",
    "    vectors_array = np.array(vectors)\n",
    "    avg_vector = np.mean(vectors_array, axis=0)\n",
    "    \n",
    "    return avg_vector\n",
    "\n",
    "# Vectorize every speech \n",
    "df_data['Vectorized_Speech'] = df_data['Speech'].apply(tokenize)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
