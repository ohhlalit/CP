{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the libraries \n",
    "import nltk \n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.corpus import stopwords\n",
    "import re\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import numpy as np \n",
    "import gensim\n",
    "from gensim.models import Word2Vec\n",
    "import pandas as pd "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset \n",
    "df_data = pd.read_csv(\"E:\\Tarang\\Ashoka\\Python\\PYTHON PROJECT\\PM_Modi_Speech_Text_english.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initializing the model \n",
    "lemmatizer = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(speech):\n",
    "    # Sentence Tokenization\n",
    "    sentence = nltk.sent_tokenize(speech)\n",
    "\n",
    "    # Creating an empty list to store the cleaned dataset \n",
    "    corpus = []\n",
    "\n",
    "    # Loop through each sentence in the speech\n",
    "    for i in range(len(sentence)):\n",
    "        # Remove any non-alphabetic characters in the sentence and convert to lowercase\n",
    "        review = re.sub(\"^[a-zA-Z0-9]+$\", \" \", sentence[i])\n",
    "        review = review.lower()\n",
    "        \n",
    "        # Tokenize the sentence into a list of words\n",
    "        review = review.split()\n",
    "        \n",
    "        # Lemmatize each word in the sentence and remove stop words\n",
    "        lemmatizer = WordNetLemmatizer()\n",
    "        review = [lemmatizer.lemmatize(word) for word in review if word not in set(stopwords.words(\"english\"))]\n",
    "        \n",
    "        # Convert the list of words back into a sentence\n",
    "        review = ' '.join(review)\n",
    "        \n",
    "        # Add the cleaned sentence to the corpus\n",
    "        corpus.append(review)\n",
    "\n",
    "    # Tokenize each sentence in the corpus into a list of words\n",
    "    corpus_tokenized = [nltk.word_tokenize(sentence) for sentence in corpus]\n",
    "\n",
    "    # Train the Word2Vec model on the tokenized corpus\n",
    "    model = Word2Vec(sentences=corpus_tokenized, vector_size=300, window=5, min_count=1, workers=4)\n",
    "\n",
    "    # Extract the word vectors for each word in the corpus\n",
    "    vectors = []\n",
    "    for sentence in corpus_tokenized:\n",
    "        for word in sentence:\n",
    "            if word in model.wv.key_to_index:\n",
    "                # If the word is in the Word2Vec model's vocabulary, extract its vector\n",
    "                word_vector = model.wv.get_vector(word)\n",
    "                vectors.append(word_vector)\n",
    "                print(word_vector)\n",
    "            else:\n",
    "                # If the word is not in the vocabulary, print a message to the console\n",
    "                print(f\"{word} not in vocabulary.\")\n",
    "    \n",
    "    # Calculate the average vector for the entire speech\n",
    "    ## Get the sentence level embeddings for a given corpus \n",
    "    corpus_vector = []\n",
    "    # for sentence in \n",
    "    vectors_array = np.array(vectors)\n",
    "    avg_vector = np.mean(vectors_array, axis=0) # Instead, take out the average the array of each word.  \n",
    "    \n",
    "    return avg_vector\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-1.7187477e-03 -2.2227792e-03 -2.5922784e-03  2.7702427e-03\n",
      " -6.6078064e-04 -2.2849878e-03 -1.3847963e-03  1.7147087e-03\n",
      " -9.5637876e-04 -1.2498867e-03  5.4047507e-04 -9.2543126e-04\n",
      " -5.2812020e-04  3.5816390e-04 -9.9264819e-04  2.8397609e-03\n",
      "  1.3036450e-03 -3.3196211e-03  2.0865332e-03 -2.2514181e-03\n",
      "  2.5647759e-04  1.4680783e-03 -1.7011249e-03 -7.0355652e-04\n",
      "  2.6984930e-03 -1.4145974e-03 -2.5454203e-03  3.0859709e-03\n",
      " -7.1849901e-04 -1.5731442e-03  2.8569333e-03  1.4277796e-03\n",
      "  1.4416123e-03  3.0948378e-03 -2.8176936e-03  1.7517734e-03\n",
      "  6.7978381e-04  1.3960934e-03  5.6596677e-04  1.4880446e-03\n",
      "  1.4954301e-03  2.0348406e-03 -1.0673658e-03 -1.5252427e-03\n",
      " -1.4217218e-04  8.4457797e-04 -1.0877224e-03  2.0192394e-03\n",
      "  1.3847101e-03  2.5881974e-03  8.5642416e-04  2.7055610e-03\n",
      " -4.6240329e-04  2.6926426e-03  1.2390054e-03 -2.6824414e-03\n",
      " -1.3112049e-03 -8.2395953e-04  1.6310147e-03 -2.9071968e-04\n",
      " -9.4363611e-04  2.6112357e-03  3.1076337e-03 -5.3830940e-04\n",
      " -1.7197498e-03 -1.5672529e-03 -1.6153503e-03 -3.2009420e-03\n",
      "  4.5734009e-04 -1.4083051e-03  8.4223627e-04  1.8714945e-03\n",
      " -1.3553019e-03 -3.1988604e-03  5.1556586e-04 -2.2333742e-03\n",
      "  8.3172478e-04 -1.2602106e-03  2.3594736e-03  2.1340688e-04\n",
      "  1.1869796e-03 -9.1304461e-04 -5.7018240e-04  2.5509309e-03\n",
      "  4.6922604e-04 -1.9501495e-03 -2.6114988e-03  4.1089574e-04\n",
      "  2.1515437e-03  1.8521165e-03 -2.9923499e-03  2.8640549e-03\n",
      "  1.3489926e-03  2.4898683e-03  3.2487782e-03 -2.4298599e-03\n",
      " -3.0133207e-03  1.9453334e-03  3.1304040e-03  1.1689750e-03\n",
      "  2.3629323e-03 -5.2264333e-04  2.6491662e-03 -3.1628862e-03\n",
      " -2.6764998e-03 -2.2134571e-03 -1.3344848e-03  1.6630722e-03\n",
      " -1.2711863e-03 -2.7733017e-03  2.8039257e-03 -1.2490007e-03\n",
      "  2.8695655e-03 -1.6319172e-03  1.3061981e-03  1.6406723e-03\n",
      "  7.9753640e-04 -9.3960128e-04  9.4970822e-04 -2.7520787e-03\n",
      " -9.2184660e-04 -8.6371938e-04  2.4163353e-03 -1.1544677e-03\n",
      " -2.1999010e-03  1.4468089e-03 -1.5816173e-04 -1.1991855e-03\n",
      "  2.2941574e-03  1.2907707e-03 -1.3000671e-03  2.5729617e-04\n",
      "  3.0478342e-03  2.5848853e-03  2.1206241e-03  1.5557675e-03\n",
      "  7.9482992e-04 -6.1387540e-04 -2.1237643e-03 -1.0060350e-04\n",
      " -5.2179617e-04 -1.9076189e-04 -2.0876236e-03  2.4780158e-03\n",
      " -2.1971643e-03 -2.4130924e-03 -9.1904879e-04 -5.0513347e-04\n",
      " -2.5452392e-03  2.3274700e-04 -1.7753704e-03 -4.2518138e-04\n",
      " -2.4550371e-03  6.5352279e-04  1.0910662e-03 -7.7128407e-06\n",
      " -1.8161193e-03 -5.7536206e-04  2.3616389e-03  1.2454196e-03\n",
      " -2.9603497e-03 -1.1378503e-03  7.8470074e-04  7.1267324e-04\n",
      " -3.1546692e-03  1.5237220e-03 -2.8856660e-03 -2.4623561e-03\n",
      "  1.1610372e-03 -1.1569861e-03  1.1881570e-03  2.9646968e-03\n",
      " -1.1914409e-03  3.1068083e-03  5.7034614e-04  3.2825915e-03\n",
      "  1.9016811e-03 -3.0498279e-03 -1.1092436e-03  2.1767251e-03\n",
      "  1.8675931e-03  2.9018386e-03  2.3087009e-03  2.6796293e-03\n",
      " -3.2743362e-03  1.4329418e-03 -1.6766922e-03  1.1707954e-03\n",
      "  2.0188959e-03  1.4640439e-03  2.5041199e-03  4.9923855e-04\n",
      " -4.2164724e-04  1.9228001e-03 -1.8798558e-03  1.2863875e-05\n",
      "  3.1521956e-03 -1.8270834e-03  1.2714263e-03 -2.7043403e-03\n",
      "  3.2567645e-03  2.7217045e-03  4.2699059e-04  1.6991929e-03\n",
      "  4.6937625e-04 -2.1517205e-03 -4.7601701e-04  2.1497218e-03\n",
      " -1.5391019e-03 -1.3310218e-03  1.6414683e-03  9.0436620e-04\n",
      " -6.1599177e-04 -9.5898111e-04  2.0035771e-03 -1.9055796e-03\n",
      " -1.0789009e-03 -2.1626083e-03 -1.4115440e-03 -2.8603314e-03\n",
      " -1.4899297e-03 -2.8370766e-03  4.6792586e-04 -2.8727320e-03\n",
      " -3.3055521e-03 -2.7338751e-03 -2.2575553e-03  2.2268617e-03\n",
      "  1.2615188e-03  1.1872212e-04 -9.8599389e-04 -2.4761069e-03\n",
      "  1.7780621e-04  1.6663075e-04  6.5206288e-05  2.8419853e-04\n",
      "  2.6211023e-04 -2.2720098e-05 -2.6690180e-03 -1.9567576e-03\n",
      " -2.7943039e-03 -4.3734748e-04  6.0687901e-04  2.4723760e-03\n",
      " -6.5447571e-04 -7.7509723e-04  3.1623847e-03  2.6568174e-05\n",
      " -8.0150721e-04  2.8682824e-03  8.9566788e-04 -1.7813242e-03\n",
      "  2.1960353e-03  1.5033846e-03 -2.3514892e-03 -1.0772466e-04\n",
      "  2.7816216e-04  1.9157858e-03 -5.7255151e-04 -9.3550998e-04\n",
      "  5.8281026e-04  2.8239051e-04  3.9760908e-04 -8.7809406e-04\n",
      " -1.9952615e-03  2.4409946e-03  2.5291252e-03  2.7654525e-03\n",
      " -2.8662824e-03  8.7880850e-04 -1.1866542e-03  3.2068014e-03\n",
      "  9.6792262e-04  1.5470377e-03  7.9520507e-04  2.2028259e-03\n",
      " -1.9144301e-03  2.6314708e-03 -8.0364069e-04 -1.5206286e-03\n",
      " -6.8699679e-04  3.2445192e-03 -2.2855301e-03 -7.3057332e-04\n",
      "  2.3336664e-03 -1.8583140e-05 -2.0983224e-03 -2.1311752e-03\n",
      "  2.9801317e-03  2.1431919e-03  1.5911977e-03 -1.0873493e-03\n",
      " -3.0892065e-03  1.2622960e-03  2.3868501e-03 -1.8776298e-03\n",
      " -2.6216710e-03 -9.9091337e-04 -1.6439661e-03 -7.7170372e-04]\n",
      "[ 2.71075731e-03 -1.48577814e-03 -3.56119068e-04  3.35454941e-04\n",
      " -6.37046469e-05  3.82725790e-04  2.03795359e-03 -6.75718002e-06\n",
      " -1.08198845e-03 -5.03576186e-04  1.96576631e-03  5.04700758e-04\n",
      " -2.41420668e-04  3.11108236e-03 -1.64042786e-03 -2.79469881e-04\n",
      "  3.05847055e-03  2.24980921e-03  5.00952010e-04 -2.96085351e-03\n",
      "  3.82915343e-04 -7.62751908e-04  3.12274578e-03  4.03309270e-04\n",
      "  4.96687891e-04  8.02136667e-04 -6.12002215e-04 -1.66654470e-03\n",
      "  7.74764994e-05 -6.71393471e-04  2.20031105e-03  2.98004108e-03\n",
      " -2.24918127e-04  9.92338290e-04 -2.03588489e-03  5.66441624e-04\n",
      " -2.30874424e-03 -2.89800880e-03 -1.96673442e-03 -2.98549165e-03\n",
      "  2.42586504e-03 -1.92401046e-03  2.75878399e-03 -2.41451501e-03\n",
      "  1.14055828e-03  3.22499941e-03 -2.59514921e-03 -3.31501919e-03\n",
      " -1.44304871e-03 -8.94376833e-04 -9.04297849e-05 -2.94385036e-03\n",
      " -2.87251920e-03  9.33403557e-04 -2.73546902e-03 -3.02311219e-03\n",
      " -7.80155242e-04 -2.87726917e-03 -2.35221675e-03 -2.80038361e-03\n",
      " -1.00442965e-04 -1.52143277e-03  2.20905826e-03  5.09053469e-04\n",
      " -1.11382524e-03  2.03632400e-03 -2.00442830e-03 -1.55205652e-03\n",
      " -2.40250304e-03 -1.44552672e-03 -6.03110006e-04  2.16321438e-03\n",
      " -9.23464308e-04  1.63965579e-03  2.30148085e-03 -2.48790183e-03\n",
      "  1.52161682e-03  2.04232614e-03 -9.84824845e-04  2.20834068e-03\n",
      "  2.04195967e-03 -2.14782823e-03 -2.25485046e-03  8.46319599e-04\n",
      " -5.41272981e-04 -2.02170922e-03  3.16640292e-03 -1.71004888e-03\n",
      " -2.18469906e-03 -3.99617347e-05 -9.00476007e-04  1.48133433e-04\n",
      " -1.17915275e-03 -1.39776865e-04 -2.36205262e-04  2.74273567e-04\n",
      "  2.73160567e-03 -1.91223586e-03 -5.53175982e-04  1.85720250e-03\n",
      "  2.72270688e-03 -1.48101093e-03  2.99514458e-03  2.75122165e-03\n",
      " -1.47840742e-03  1.01035039e-04  1.42483041e-03 -1.30877341e-03\n",
      " -1.85332180e-03 -2.17077415e-03 -2.23579409e-04 -9.86405212e-05\n",
      "  1.48769503e-03 -8.24684685e-04 -5.75363629e-05  8.20625224e-04\n",
      "  1.62253296e-03 -1.02694830e-05 -2.11313646e-03 -3.08693573e-03\n",
      "  8.88586055e-06  2.22063134e-03  4.88674268e-04 -2.98884069e-03\n",
      " -2.64620152e-03  2.18396750e-03 -1.26189354e-03  2.08499748e-03\n",
      " -2.22701067e-03  2.82655400e-03 -2.17210804e-03  1.09600660e-03\n",
      " -3.52328614e-04 -2.26250920e-03 -1.09586562e-03 -3.87137319e-04\n",
      " -1.82364660e-03 -4.03782527e-04 -2.52110441e-03  8.82219872e-04\n",
      "  3.02338274e-03 -7.92416744e-04 -3.25503352e-04  1.17118715e-03\n",
      "  2.88836239e-03 -1.97395077e-03 -2.29585916e-03 -9.77661577e-04\n",
      "  3.04923207e-03  2.88755895e-04 -2.89280014e-03 -4.82326344e-04\n",
      "  3.15982173e-03 -2.51649576e-03 -1.78603292e-03  3.10552074e-03\n",
      " -2.99124210e-03  1.27530261e-03  2.21813520e-04  2.22023367e-03\n",
      "  2.77091772e-03 -9.50261776e-04 -1.33077102e-03  2.96597253e-03\n",
      "  6.96548610e-04  2.08298047e-03 -3.14857159e-03  3.19670793e-03\n",
      " -4.49436106e-04 -2.01737159e-03  9.97511554e-04 -1.52203633e-04\n",
      "  1.56883080e-03 -7.61007075e-04 -1.37928093e-03  7.59299612e-04\n",
      "  2.78479466e-03 -1.66520197e-03  8.89559567e-04 -2.66351830e-03\n",
      " -2.25778227e-03 -1.55889589e-04 -2.92257592e-03  9.29812610e-04\n",
      "  5.32865117e-04 -7.73230800e-04  1.66793028e-03  3.24959564e-03\n",
      "  2.81808921e-03 -6.26741676e-04  6.86050626e-04 -1.33456313e-03\n",
      " -2.74713524e-03  2.09265202e-03 -6.49727182e-04 -2.22068236e-04\n",
      " -5.90444019e-04 -1.51188846e-03  1.35390321e-03 -1.42339349e-03\n",
      " -3.19284876e-03  2.98103853e-03  1.38835632e-03  3.07824486e-03\n",
      "  2.21450091e-03  9.74912255e-04  3.26800672e-03 -1.47488038e-03\n",
      " -2.26777047e-03  1.40912691e-03  1.24300004e-03 -1.88820367e-03\n",
      "  3.23492009e-03 -1.18610228e-03  3.18313553e-03  2.78242020e-04\n",
      " -2.11281888e-03 -6.59039011e-04 -2.45901826e-03 -9.93174268e-04\n",
      "  3.47232417e-04  3.16089601e-03  3.11861583e-03 -2.19862582e-03\n",
      "  1.15838367e-03  7.58523529e-04 -8.29784083e-04 -3.07639083e-03\n",
      "  3.42375424e-04 -2.72190222e-03  2.10672966e-03 -1.93336012e-03\n",
      "  1.84514641e-03  3.27790785e-03 -5.33334423e-05  1.50949752e-03\n",
      " -6.03133463e-04  2.45358702e-03  1.31336565e-03 -3.00344150e-03\n",
      " -7.99501315e-04  1.20958965e-03 -3.31894553e-05 -4.00423596e-04\n",
      " -3.51812836e-04 -5.57200518e-04  2.01650852e-04  1.38836505e-03\n",
      " -1.41759717e-03 -1.27787387e-03 -1.76056219e-05  8.97852588e-05\n",
      " -5.62687710e-05 -1.59516896e-03  1.43780070e-03 -7.23973091e-04\n",
      "  7.01179903e-04  2.22174320e-04  1.98989222e-03 -2.28079362e-03\n",
      " -2.27190345e-03 -1.49208587e-03  3.14527634e-03 -5.30627556e-04\n",
      " -3.14308098e-03 -1.81680516e-04 -1.48297427e-03  2.00002640e-03\n",
      " -3.19456169e-03  9.53000388e-04 -3.08427727e-03  4.16600320e-04\n",
      "  1.99973257e-03  2.46578245e-03 -2.54048780e-03 -2.01767450e-03\n",
      " -2.27948022e-03 -2.63944664e-03 -3.16636008e-03 -7.08498934e-04\n",
      " -2.78644176e-04 -2.41873390e-03  2.26234552e-03  3.73206538e-04\n",
      "  1.94295566e-03  4.90955485e-04  2.63121910e-04 -2.45604315e-03\n",
      " -7.25552614e-04  1.44035975e-03 -1.69510487e-03  3.76929849e-04\n",
      "  9.61121346e-04 -5.12120314e-04  3.31076514e-03  2.78321141e-03\n",
      "  8.05222197e-04  2.37274845e-03  1.96381263e-03 -1.86020578e-03]\n",
      "[-2.7475592e-03  3.0997850e-03 -6.5886976e-05 -6.5575878e-04\n",
      "  1.5345435e-03 -1.3651053e-03  9.1437140e-04  2.3133222e-03\n",
      "  2.0218086e-03 -2.5035981e-03  3.1274501e-03  1.5572695e-03\n",
      "  1.3220402e-03 -2.0811686e-03  2.8199931e-03 -7.1672164e-04\n",
      "  2.9417293e-03 -1.7873343e-03 -2.7098064e-03  2.2748529e-03\n",
      "  5.5706420e-04 -7.3283631e-04  3.1712004e-03  3.1646185e-03\n",
      " -3.2580157e-03  8.3507615e-04  2.0522308e-03  1.2908188e-03\n",
      "  6.7426247e-04  1.4350057e-04  2.2454381e-04 -1.2735454e-03\n",
      " -2.3800833e-03 -6.9629075e-04  1.3079660e-03  2.9395611e-03\n",
      "  3.0863835e-03 -1.9919788e-03 -3.1342236e-03  3.2547922e-03\n",
      "  1.1432616e-03  1.7220390e-03  2.0941151e-03 -9.3475421e-04\n",
      "  2.4409012e-03  9.4342389e-04  9.5700147e-04 -7.9345662e-04\n",
      " -1.0427498e-03 -7.9004723e-04  1.4254788e-03  2.5352638e-05\n",
      " -3.1947596e-03 -3.2218480e-03 -2.0493979e-03 -4.2856533e-05\n",
      "  6.6580536e-04  3.1439892e-03  1.8614503e-03 -1.4302321e-03\n",
      "  9.2772243e-05  1.6547863e-03  2.5661031e-03 -3.8140774e-04\n",
      "  1.4411402e-03 -1.9381265e-03 -2.6806354e-04  2.7000168e-03\n",
      " -7.8668835e-04 -3.2211519e-03  1.9264201e-03 -1.3099408e-03\n",
      " -4.0762423e-04  3.3268393e-03 -7.5211684e-04 -1.5856882e-03\n",
      " -1.7764624e-03  2.3269632e-03 -1.9029573e-03  7.0455432e-04\n",
      " -1.7518866e-03  2.0402379e-03  1.4524356e-03  8.6878496e-04\n",
      " -4.9702765e-04 -9.1535447e-04  2.9976456e-03  1.7385916e-03\n",
      " -7.2083989e-04 -3.1567700e-03 -2.4753506e-03 -3.5458049e-04\n",
      " -2.6498237e-04 -8.5430307e-04  3.2275736e-03 -1.5284022e-04\n",
      "  1.9579204e-03 -2.4825290e-03 -8.3535793e-04 -1.8499545e-03\n",
      " -2.3796717e-03  4.1367690e-04 -2.3922387e-03 -7.4820599e-04\n",
      "  1.2397679e-03  1.9443746e-03  3.9939443e-04  7.0091052e-04\n",
      " -1.3701303e-03  2.4084444e-03 -2.1023473e-03  1.5490719e-03\n",
      " -2.7399911e-03  6.7882257e-04 -1.6590174e-03 -1.4158960e-03\n",
      " -1.0363280e-03  1.8850697e-03  1.9328002e-03 -1.6582164e-03\n",
      "  2.5777696e-04 -2.8319259e-03  2.6032687e-03  3.0857639e-03\n",
      " -9.1410917e-04  2.6674112e-04  2.4888397e-04  1.8259616e-03\n",
      " -2.8686929e-03  1.9481858e-04  2.2898074e-03  7.4386480e-04\n",
      "  3.7489214e-04 -3.1073852e-03  2.8274555e-03 -2.0880424e-03\n",
      " -9.9745789e-04  1.1645957e-03 -2.5754253e-04  4.7043044e-04\n",
      "  5.9399725e-04 -2.2762998e-03 -3.2416037e-03  3.0135282e-03\n",
      "  2.0660183e-03 -2.3043093e-03  1.1344942e-03  6.8687994e-05\n",
      "  1.5845819e-03 -2.3733142e-03  1.3423180e-03  1.4491447e-03\n",
      "  3.3191233e-03 -1.4912466e-03 -4.6308796e-04 -2.4391070e-03\n",
      " -3.2326097e-03 -3.0267525e-03 -3.4091831e-04 -2.1677634e-03\n",
      "  1.6165761e-03 -2.0546755e-03  8.3972851e-04  2.4648031e-04\n",
      " -1.1307180e-03 -3.2640775e-04  3.3263750e-03  3.0486290e-03\n",
      " -1.4872765e-03  3.0276754e-03 -1.8805878e-03  1.9769741e-03\n",
      " -1.0324061e-03  1.1439172e-03  1.0057421e-03  2.3001535e-03\n",
      " -7.9129456e-04  2.9250120e-03  2.5298095e-03 -3.1825488e-03\n",
      " -2.6694031e-03 -2.5459656e-03  9.7441912e-04 -9.3157409e-04\n",
      " -2.3098402e-03 -2.7094213e-03  2.7697266e-03  6.6349626e-04\n",
      " -3.1093392e-03 -1.5975721e-03  1.0455796e-03 -1.5710688e-03\n",
      "  1.7602809e-03 -1.4111471e-03  8.8059861e-04 -2.6818959e-03\n",
      "  2.0699620e-03  1.6062963e-03  2.6239752e-04  1.0044825e-03\n",
      " -2.9091609e-03  7.1005384e-04 -2.9118141e-04 -3.1063629e-03\n",
      " -3.1427140e-03 -4.7023932e-04  1.4774696e-03  1.2346903e-03\n",
      " -2.1662312e-03 -2.2910226e-03 -1.6664708e-03 -7.6228142e-04\n",
      " -2.4167625e-03 -3.2011061e-03 -9.1454305e-04 -2.7876135e-03\n",
      " -2.0129585e-03 -1.8903096e-03 -7.8137912e-04 -5.6899904e-04\n",
      " -2.9856663e-03 -2.4506648e-04  2.7175022e-03  2.5634766e-03\n",
      " -2.4020385e-03 -1.2222771e-03  1.0395173e-03 -3.1902408e-03\n",
      "  4.9214641e-04  2.1748221e-03  1.9154731e-03 -2.9210206e-03\n",
      " -1.5057146e-03 -2.7133869e-03  1.5318790e-05  3.0878778e-03\n",
      "  1.9911020e-03  1.6891026e-03  1.6870209e-03 -1.0809724e-03\n",
      "  3.1840610e-03 -2.4521414e-03 -2.4234625e-03 -7.5512967e-04\n",
      " -2.5952022e-04 -1.0720345e-03 -1.9752860e-04  2.4962744e-03\n",
      " -2.3250619e-04 -5.4164691e-04  9.1479975e-04 -2.7863670e-03\n",
      "  2.6186013e-03  2.8453679e-03 -3.1946958e-03  8.1542216e-04\n",
      "  3.3016570e-03 -2.5552679e-03 -2.3223062e-03 -2.5788390e-03\n",
      "  2.7986411e-03 -2.2711197e-04  3.0481361e-03 -2.7194070e-03\n",
      "  1.2476948e-03  8.7834755e-04  2.4757106e-04  7.7589194e-04\n",
      " -2.4896979e-03 -3.1194580e-03  7.8485889e-04  2.0494850e-03\n",
      "  2.6618962e-03  1.9119648e-03 -2.5911213e-04  2.7687217e-03\n",
      " -3.1121047e-03  1.1353774e-03  8.8917812e-05  1.2857481e-03\n",
      "  2.4619277e-03 -2.2417223e-03  1.8614936e-03 -3.1740749e-03\n",
      " -2.6815294e-04 -2.8962458e-03 -1.6995576e-03  3.0964089e-03\n",
      " -6.1942061e-04  9.7147544e-04  3.0237599e-03  2.9793775e-03\n",
      " -2.7361449e-03 -1.0041046e-03  3.2955352e-03  1.7014770e-03\n",
      " -5.2936235e-04 -2.8973406e-03  9.8717213e-04 -2.2252994e-03]\n",
      "[-1.78742412e-04  7.88104517e-05  1.70111656e-03  3.00309109e-03\n",
      " -3.10098333e-03 -2.37226952e-03  2.15295749e-03  2.99099600e-03\n",
      " -1.67180935e-03 -1.25445728e-03  2.46016821e-03 -5.11157094e-04\n",
      " -1.51220441e-03  2.18468392e-03 -1.62005343e-03 -6.05339184e-04\n",
      "  9.58859921e-04  3.30624578e-04 -2.76173837e-03 -3.14960605e-03\n",
      "  2.43725535e-03  1.69008737e-03  2.25256453e-03  2.54288519e-04\n",
      "  2.11696350e-03 -1.13512203e-03 -3.15467129e-04  1.92285771e-03\n",
      " -2.50721257e-03 -1.31203455e-03 -2.50386074e-03 -3.10014089e-04\n",
      "  3.17937299e-03 -2.43972219e-03 -7.77922862e-04 -6.45913708e-04\n",
      "  2.69247894e-03 -1.97696523e-03  1.50541464e-05 -1.58457796e-03\n",
      " -3.20118340e-03  1.66909769e-03 -2.91986181e-03 -1.46394176e-03\n",
      " -1.16999945e-05 -9.87271487e-05 -2.55374680e-03  3.20491428e-03\n",
      "  1.66068599e-03  3.07771447e-03 -2.71930569e-03  1.49859942e-03\n",
      " -1.37902540e-03  2.74845370e-04  2.83287326e-03 -1.48739223e-03\n",
      "  1.50583347e-03 -2.26232014e-03 -1.18282950e-03  3.13283596e-03\n",
      " -5.25884214e-04  1.07123851e-04 -1.38020993e-03 -2.56089610e-03\n",
      " -5.02669427e-04  8.23264942e-04 -2.96008977e-04  1.84455398e-03\n",
      " -9.14325705e-04  7.53355038e-04  1.81859813e-03  2.78198440e-03\n",
      " -4.84580203e-04 -3.06938100e-03  1.45685079e-03  1.90594990e-04\n",
      "  2.48063612e-03 -2.71094235e-04 -8.79471307e-04 -2.91766971e-03\n",
      " -2.85518967e-04  9.42187733e-04  1.80047634e-03  2.35088542e-03\n",
      " -1.90104044e-03  6.19606581e-04  2.02962128e-03 -1.59935036e-03\n",
      " -1.03575352e-03  2.26587662e-03  5.43825212e-04  6.33056989e-05\n",
      "  1.15787901e-03  7.25924983e-05  3.20627540e-03  1.68686791e-03\n",
      " -2.97246338e-03 -2.34718691e-03  3.00485292e-04  2.13084463e-03\n",
      " -2.87322910e-03  1.22191268e-03  1.72996125e-03  1.91397942e-03\n",
      "  2.48897285e-03 -2.05589179e-03  3.68537905e-04  2.01576063e-03\n",
      " -9.46683460e-04 -2.05784081e-03 -1.36741000e-04 -2.78964965e-03\n",
      " -1.86667079e-03  2.36817962e-03  1.11751317e-03  2.40855664e-03\n",
      "  2.26674904e-03  2.51024729e-03 -1.26305141e-03 -1.87268655e-04\n",
      "  7.82792165e-04 -1.50634407e-03  2.79624388e-03 -3.28605459e-03\n",
      "  2.25488027e-03  9.71472240e-04 -1.64427713e-03  1.46606250e-03\n",
      " -5.79858199e-04  2.23712809e-03  3.32161668e-03 -1.45414786e-03\n",
      " -1.99779271e-04 -1.89854577e-03  1.28360745e-03  9.28875583e-04\n",
      "  2.29702541e-03  2.03369861e-03  3.17949895e-03  3.09113902e-03\n",
      "  2.63268943e-03 -2.32983474e-03 -3.05195502e-03 -1.18584234e-04\n",
      " -1.03328028e-03  2.63143890e-03  1.97952474e-03 -5.15220978e-04\n",
      "  5.03654475e-04  5.96680271e-04  2.60585709e-03 -3.17006232e-03\n",
      " -6.85103732e-05  1.15639891e-03 -3.12990742e-04  2.79392395e-03\n",
      "  3.00359447e-03  2.17883545e-03 -2.37207016e-04  2.57013482e-03\n",
      " -2.84477836e-03  1.06903550e-03 -1.54599908e-03 -1.69631839e-03\n",
      "  1.19653938e-03  1.79011305e-03  2.58983811e-03 -1.92216877e-03\n",
      "  2.47778697e-03  2.20849877e-03 -1.23660010e-03 -2.91521382e-03\n",
      "  1.81248900e-03  2.16991850e-03 -2.62516725e-04 -2.23661866e-03\n",
      " -2.36197514e-03 -8.32353428e-04  1.71441794e-03 -1.22174586e-03\n",
      " -3.12335324e-03  1.27557991e-03  1.62815966e-03 -2.14285450e-03\n",
      "  4.02852689e-04 -6.91625697e-04  8.13444422e-06 -3.29450285e-03\n",
      "  8.97334830e-04 -1.58336875e-03  3.62548832e-04 -5.25408192e-04\n",
      "  7.32224376e-04 -2.62719183e-03 -9.05727968e-04  8.87732895e-04\n",
      "  1.78222731e-03 -7.97171611e-04 -3.17003136e-03  1.50195952e-03\n",
      "  3.15213219e-05  1.02577324e-03 -2.27088179e-03 -4.58488474e-04\n",
      "  2.55619362e-03  2.44880328e-03 -1.22443237e-03  8.80900596e-04\n",
      " -2.77237664e-03  2.06849538e-03 -1.54577417e-03 -1.05470221e-03\n",
      "  3.10378545e-03  2.91128556e-04  2.49690097e-03 -2.02468759e-03\n",
      "  1.72016898e-03  3.30760749e-03 -2.81913043e-03 -1.71189709e-03\n",
      " -2.35494575e-03 -1.62088394e-03 -1.25952123e-03 -2.84539978e-03\n",
      "  2.65186862e-03 -1.61464617e-03  2.80787121e-03  1.75419007e-03\n",
      " -2.18334189e-03  1.31929037e-03  1.82338315e-03 -2.47551198e-03\n",
      " -2.46857316e-03 -8.25076888e-04 -2.87524192e-03 -5.27190743e-04\n",
      " -1.34477610e-04  1.09989487e-03  4.80626826e-04 -2.93807185e-04\n",
      " -1.86468603e-03  5.76788560e-04 -2.99123931e-04  2.26456369e-03\n",
      "  1.32452999e-03  1.50982384e-03  4.78101982e-04 -8.99951847e-04\n",
      " -1.45560422e-03 -3.44024884e-04  4.79009148e-04 -8.82002932e-04\n",
      " -2.35792762e-03 -2.60176905e-03 -3.04059545e-03 -1.97838992e-03\n",
      " -6.15808181e-04 -1.44129037e-03 -2.15355679e-03 -1.23910746e-03\n",
      "  1.42971950e-03 -1.24634779e-03  2.79272511e-03  5.11331134e-04\n",
      " -2.41410662e-03  3.14459950e-03  2.54373741e-03  1.83109404e-03\n",
      " -2.28294847e-03  1.94089289e-03  1.33636431e-03  1.72845647e-03\n",
      "  1.41863385e-03  6.46584842e-04 -1.05672085e-03  2.78461492e-03\n",
      "  3.20406002e-03  1.26420101e-03 -9.45665059e-04  2.37584118e-06\n",
      "  4.06272797e-04 -2.81944149e-03 -2.74131494e-03 -7.70052284e-05\n",
      "  4.12429188e-04 -1.91446021e-03 -1.57509127e-03 -2.44869152e-03\n",
      "  2.77620507e-03  4.04326129e-05 -1.50313298e-03  1.90056837e-03\n",
      "  3.06000514e-03 -1.36662403e-03  2.65489379e-03  1.79181143e-03\n",
      "  1.95970782e-03  1.70863466e-04  2.73769465e-03 -2.33968021e-03]\n"
     ]
    }
   ],
   "source": [
    "string = \"Hello WOrld! I am a machine computer.\"\n",
    "l = tokenize(string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converts the Speech column to a string data type.\n",
    "df_data['Speech'] = df_data['Speech'].astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleaned(speech):\n",
    "    # Sentence Tokenization\n",
    "    sentence = nltk.sent_tokenize(speech)\n",
    "\n",
    "    # Creating an empty list to store the cleaned dataset \n",
    "    corpus = []\n",
    "\n",
    "    # Loop through each sentence in the speech\n",
    "    for i in range(len(sentence)):\n",
    "        # Remove any non-alphabetic characters in the sentence and convert to lowercase\n",
    "        review = re.sub(\"[^a-zA-Z]\", \" \", sentence[i])\n",
    "        review = review.lower()\n",
    "        \n",
    "        # Tokenize the sentence into a list of words\n",
    "        review = review.split()\n",
    "        \n",
    "        # Lemmatize each word in the sentence and remove stop words\n",
    "        lemmatizer = WordNetLemmatizer()\n",
    "        review = [lemmatizer.lemmatize(word) for word in review if word not in set(stopwords.words(\"english\"))]\n",
    "        \n",
    "        # Convert the list of words back into a sentence\n",
    "        review = ' '.join(review)\n",
    "        \n",
    "        # Add the cleaned sentence to the corpus\n",
    "        corpus.append(review)\n",
    "\n",
    "    return corpus "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_data['Cleaned_Speech'] = df_data['Speech'].apply(cleaned)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vectorize every speech \n",
    "df_data['Vectorized_Speech'] = df_data['Speech'].apply(tokenize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "300"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df_data['Vectorized_Speech'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>Date</th>\n",
       "      <th>Speech</th>\n",
       "      <th>Title</th>\n",
       "      <th>Vectorized_Speech</th>\n",
       "      <th>Cleaned_Speech</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2</td>\n",
       "      <td>09 JAN 2017</td>\n",
       "      <td>Chief Minister of Gujarat Shri Vijay Rupani ji...</td>\n",
       "      <td>Text of PM's address at the Inauguration of No...</td>\n",
       "      <td>[5.5015735e-05, 0.00032888833, 0.00011379553, ...</td>\n",
       "      <td>[chief minister gujarat shri vijay rupani ji c...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3</td>\n",
       "      <td>09 JAN 2017</td>\n",
       "      <td>I  am delighted to be here at Gift City to ina...</td>\n",
       "      <td>Text of PM’s address on the occasion of Inaugu...</td>\n",
       "      <td>[-1.9600202e-05, 0.0004415475, 0.00011244467, ...</td>\n",
       "      <td>[delighted gift city inaugurate india first in...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5</td>\n",
       "      <td>03 JAN 2017</td>\n",
       "      <td>Governor of Andhra Pradesh, Shri E. S. L. Nara...</td>\n",
       "      <td>PM's Address at the Inauguration of the 104th ...</td>\n",
       "      <td>[-2.6159776e-05, 0.0005052889, -0.00010802128,...</td>\n",
       "      <td>[governor andhra pradesh shri e l narasimhan c...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>6</td>\n",
       "      <td>26 FEB 2017</td>\n",
       "      <td>My dear countrymen, Namaskar. Winter is on its...</td>\n",
       "      <td>English Translation of the text of PM’s ‘Mann ...</td>\n",
       "      <td>[-1.18660855e-05, 0.00044458942, 6.186281e-05,...</td>\n",
       "      <td>[dear countryman namaskar, winter way, vasant ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>8</td>\n",
       "      <td>21 FEB 2017</td>\n",
       "      <td>Namaskaram. Greetings to everyone. Swami Nirvi...</td>\n",
       "      <td>Text of PM’s inaugural address (via video conf...</td>\n",
       "      <td>[-4.466092e-05, 0.00029940152, -5.0204777e-05,...</td>\n",
       "      <td>[namaskaram, greeting everyone, swami nirvinan...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1177</th>\n",
       "      <td>1263</td>\n",
       "      <td>08 APR 2023</td>\n",
       "      <td>Bharat Mata Ki Jai!\\n\\nBharat Mata Ki Jai!\\n\\n...</td>\n",
       "      <td>English rendering of PM’s address at launch of...</td>\n",
       "      <td>[-2.997346e-05, 0.00049227197, 8.608076e-05, 0...</td>\n",
       "      <td>[bharat mata ki jai, bharat mata ki jai, gover...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1178</th>\n",
       "      <td>1264</td>\n",
       "      <td>04 APR 2023</td>\n",
       "      <td>ExcellenciesHonourable Minister Mr. Harbers;Sp...</td>\n",
       "      <td>Text of Address by Dr. P.K. Mishra, Principal ...</td>\n",
       "      <td>[4.4897748e-05, 0.00028738583, 1.590124e-05, 0...</td>\n",
       "      <td>[excellencieshonourable minister mr harbers sp...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1179</th>\n",
       "      <td>1265</td>\n",
       "      <td>04 APR 2023</td>\n",
       "      <td>Namaskar!Excellencies, heads of state, Academi...</td>\n",
       "      <td>Text of PM’s remarks at International Conferen...</td>\n",
       "      <td>[3.0073044e-05, 0.00033384794, 5.3663907e-06, ...</td>\n",
       "      <td>[namaskar excellency head state academic busin...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1180</th>\n",
       "      <td>1266</td>\n",
       "      <td>03 APR 2023</td>\n",
       "      <td>My colleague in the Union Cabinet Dr. Jitendra...</td>\n",
       "      <td>English rendering of PM’S address at the diamo...</td>\n",
       "      <td>[-4.503283e-05, 0.00028559507, 1.2794534e-05, ...</td>\n",
       "      <td>[colleague union cabinet dr jitendra singh ji ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1181</th>\n",
       "      <td>1267</td>\n",
       "      <td>01 APR 2023</td>\n",
       "      <td>Bharat Mata Ki Jai!Bharat Mata Ki Jai!The Gove...</td>\n",
       "      <td>English rendering of PM’s speech at flagging o...</td>\n",
       "      <td>[-1.2360649e-05, 0.00041997337, -3.8412763e-05...</td>\n",
       "      <td>[bharat mata ki jai bharat mata ki jai governo...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1182 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      Unnamed: 0         Date  \\\n",
       "0              2  09 JAN 2017   \n",
       "1              3  09 JAN 2017   \n",
       "2              5  03 JAN 2017   \n",
       "3              6  26 FEB 2017   \n",
       "4              8  21 FEB 2017   \n",
       "...          ...          ...   \n",
       "1177        1263  08 APR 2023   \n",
       "1178        1264  04 APR 2023   \n",
       "1179        1265  04 APR 2023   \n",
       "1180        1266  03 APR 2023   \n",
       "1181        1267  01 APR 2023   \n",
       "\n",
       "                                                 Speech  \\\n",
       "0     Chief Minister of Gujarat Shri Vijay Rupani ji...   \n",
       "1     I  am delighted to be here at Gift City to ina...   \n",
       "2     Governor of Andhra Pradesh, Shri E. S. L. Nara...   \n",
       "3     My dear countrymen, Namaskar. Winter is on its...   \n",
       "4     Namaskaram. Greetings to everyone. Swami Nirvi...   \n",
       "...                                                 ...   \n",
       "1177  Bharat Mata Ki Jai!\\n\\nBharat Mata Ki Jai!\\n\\n...   \n",
       "1178  ExcellenciesHonourable Minister Mr. Harbers;Sp...   \n",
       "1179  Namaskar!Excellencies, heads of state, Academi...   \n",
       "1180  My colleague in the Union Cabinet Dr. Jitendra...   \n",
       "1181  Bharat Mata Ki Jai!Bharat Mata Ki Jai!The Gove...   \n",
       "\n",
       "                                                  Title  \\\n",
       "0     Text of PM's address at the Inauguration of No...   \n",
       "1     Text of PM’s address on the occasion of Inaugu...   \n",
       "2     PM's Address at the Inauguration of the 104th ...   \n",
       "3     English Translation of the text of PM’s ‘Mann ...   \n",
       "4     Text of PM’s inaugural address (via video conf...   \n",
       "...                                                 ...   \n",
       "1177  English rendering of PM’s address at launch of...   \n",
       "1178  Text of Address by Dr. P.K. Mishra, Principal ...   \n",
       "1179  Text of PM’s remarks at International Conferen...   \n",
       "1180  English rendering of PM’S address at the diamo...   \n",
       "1181  English rendering of PM’s speech at flagging o...   \n",
       "\n",
       "                                      Vectorized_Speech  \\\n",
       "0     [5.5015735e-05, 0.00032888833, 0.00011379553, ...   \n",
       "1     [-1.9600202e-05, 0.0004415475, 0.00011244467, ...   \n",
       "2     [-2.6159776e-05, 0.0005052889, -0.00010802128,...   \n",
       "3     [-1.18660855e-05, 0.00044458942, 6.186281e-05,...   \n",
       "4     [-4.466092e-05, 0.00029940152, -5.0204777e-05,...   \n",
       "...                                                 ...   \n",
       "1177  [-2.997346e-05, 0.00049227197, 8.608076e-05, 0...   \n",
       "1178  [4.4897748e-05, 0.00028738583, 1.590124e-05, 0...   \n",
       "1179  [3.0073044e-05, 0.00033384794, 5.3663907e-06, ...   \n",
       "1180  [-4.503283e-05, 0.00028559507, 1.2794534e-05, ...   \n",
       "1181  [-1.2360649e-05, 0.00041997337, -3.8412763e-05...   \n",
       "\n",
       "                                         Cleaned_Speech  \n",
       "0     [chief minister gujarat shri vijay rupani ji c...  \n",
       "1     [delighted gift city inaugurate india first in...  \n",
       "2     [governor andhra pradesh shri e l narasimhan c...  \n",
       "3     [dear countryman namaskar, winter way, vasant ...  \n",
       "4     [namaskaram, greeting everyone, swami nirvinan...  \n",
       "...                                                 ...  \n",
       "1177  [bharat mata ki jai, bharat mata ki jai, gover...  \n",
       "1178  [excellencieshonourable minister mr harbers sp...  \n",
       "1179  [namaskar excellency head state academic busin...  \n",
       "1180  [colleague union cabinet dr jitendra singh ji ...  \n",
       "1181  [bharat mata ki jai bharat mata ki jai governo...  \n",
       "\n",
       "[1182 rows x 6 columns]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['best wish occasion pravasi bhartiya day',\n",
       " 'tradition pravasi diva first pravasi parliamentarian conference adding new chapter today',\n",
       " 'extend warm welcome friend come north america south america africa europe asia pacific region also corner world',\n",
       " 'welcome india',\n",
       " 'welcome home ancestor old memory connected different part country',\n",
       " 'ancestor went abroad business education',\n",
       " 'forcibly taken country others taken country using allurement',\n",
       " 'might physically left place left part soul mind land',\n",
       " 'land airport india part soul feel delighted seeing land',\n",
       " 'time throat get choked emotion want express form tear',\n",
       " 'people try best control outpouring emotion succeed',\n",
       " 'eye become moist due tear also lit due visit india',\n",
       " 'understand feeling',\n",
       " 'affection love respect scent air place smell land bow part',\n",
       " 'imagine happy ancestor would seeing today',\n",
       " 'wherever might happiest see land',\n",
       " 'friend india never went heart people went india last hundred year',\n",
       " 'kept alive indian civilization value whichever part world settled',\n",
       " 'surprising wherever people indian origin settled adopted place home fully integrating',\n",
       " 'one hand kept alive indian value within fully integrated language food attire country',\n",
       " 'people indian origin left impression field sport art cinema global platform',\n",
       " 'talk politics see mini world parliament people indian origin eye',\n",
       " 'today people indian origin prime minister mauritius portugal ireland',\n",
       " 'people indian origin occupied office head state head government several country',\n",
       " 'matter great pride u former president guyana shri bharat jagdev ji today present u',\n",
       " 'distinguished people also playing important role politics country',\n",
       " 'friend india motherland ancestor proud',\n",
       " 'achievement success matter pride respect u',\n",
       " 'medium report news people assuming office even news filing nomination people great readership viewership india',\n",
       " 'people affecting geo politics entire region people formulating policy respective country people india read story great interest',\n",
       " 'also discus development like look one u occupied high office',\n",
       " 'people deserve praise giving u happiness making u proud',\n",
       " 'brother sister people living different country long time',\n",
       " 'would experienced last three four year approach india changed',\n",
       " 'greater focus india approach world towards u changing',\n",
       " 'main reason behind thing india changing transforming',\n",
       " 'addition change economic social level change also come level idea well',\n",
       " 'india come long way kind thinking nothing change continue run way nothing going happen',\n",
       " 'time hope expectation indian highest level',\n",
       " 'see complete transformation system see impact irreversible change every sector thing became possible bringing far reaching policy reform every sector indian economy',\n",
       " 'guiding principle reform transform',\n",
       " 'objective make entire system transparent accountable',\n",
       " 'objective completely eradicate corruption',\n",
       " 'friend abolished web hundred tax country gst good service tax integrated country economically',\n",
       " 'single sector like mining fertilizer textile aviation health defense construction real estate food processing brought reform',\n",
       " 'friend today india youngest country world',\n",
       " 'youth unlimited dream expectation',\n",
       " 'government continuously working channelize energy right field business',\n",
       " 'scheme like skill india mission start scheme stand scheme mudra scheme started precisely reason',\n",
       " 'crore loan sanctioned mudra scheme self employment',\n",
       " 'r',\n",
       " 'lakh crore disbursed people without bank guarantee',\n",
       " 'scheme alone provided crore new entrepreneur country',\n",
       " 'government increasing investment infrastructure transport sector keeping requirement india st century mind',\n",
       " 'thing given special importance policy kind logistics india need future',\n",
       " 'highway railway airway waterway port developed manner connected support friend today new railway track india laid twice speed',\n",
       " 'doubling track taking place twice speed',\n",
       " 'national highway built twice speed',\n",
       " 'double new renewable energy capacity connected grid',\n",
       " 'cargo handling growth shipping industry negative earlier government registered growth',\n",
       " 'new employment opportunity generated due effort',\n",
       " 'small scale industry getting new work local level',\n",
       " 'example talk ujjwala scheme limited providing free gas connection poor woman',\n",
       " 'scheme given relief kitchen smoke three crore woman',\n",
       " 'helped making state kerosene free one advantage also',\n",
       " 'new cooking gas dealer appointed number gas delivery men gone launch ujjwala scheme',\n",
       " 'mean addition social reform economic empowerment society also taking place',\n",
       " 'brother sister culture belief whole world family given lot world',\n",
       " 'gone united nation first time put forward proposal international yoga day world',\n",
       " 'aware within day proposal cleared unanimously co sponsored country',\n",
       " 'today way million million people around world celebrate st june yoga day matter pride u',\n",
       " 'method holistic living gift rich tradition india',\n",
       " 'friendsi along president france proposed form international solar alliance time paris agreement climate change',\n",
       " 'become reality',\n",
       " 'making global platform solar technology financing help solar rich country',\n",
       " 'method living maintaining balance nature also indian contribution age',\n",
       " 'brother sister earthquake struck nepal flood sri lanka shortage water maldives india reached first responder',\n",
       " 'crisis yemen safely evacuated citizen also evacuated citizen country',\n",
       " 'protecting human value even time grave crisis part indian tradition considering entire world one family',\n",
       " 'friend hundred year world war',\n",
       " 'lakh indian soldier sacrificed life first second world war',\n",
       " 'india directly involved war',\n",
       " 'india interest single inch land country world war',\n",
       " 'world recognize great sacrifice made india',\n",
       " 'tradition continued even independence',\n",
       " 'india one among nation make maximum contribution un peace keeping force',\n",
       " 'india message sacrifice world human value peace',\n",
       " 'renunciation self interest feeling service renunciation identity',\n",
       " 'india special acceptance world due human value',\n",
       " 'along india society people indian origin people also special acceptance',\n",
       " 'friend whenever visit country always try meet people indian origin living country',\n",
       " 'fortunate meet visit',\n",
       " 'biggest reason behind effort believe permanent ambassador promoting india relation people indian origin',\n",
       " 'constant endeavor regular touch non resident indian continue solve problem',\n",
       " 'earlier separate ministry overseas indian received feedback non resident indian problem coordination ministry external affair',\n",
       " 'merged ministry one getting feedback people',\n",
       " 'would aware earlier two distinct scheme pio oci people even aware distinction two',\n",
       " 'simplified process created one scheme merging',\n",
       " 'minister external affair sushma swaraj ji keep eye problem indian citizen also keep eye people indian origin x find active',\n",
       " 'leadership minister external affair created madad portal real time monitoring extending help consular grievance',\n",
       " 'present pravasi bhartiya day organised every alternate year',\n",
       " 'addition regional pravasi bhartiya day also organised',\n",
       " 'sushma ji come back participating conference singapore',\n",
       " 'brother sister today building present building dedicated people indian origin nd october',\n",
       " 'matter great happiness short span time centre emerged hub people indian origin',\n",
       " 'urge people must visit exhibition life mahatma gandhi organised',\n",
       " 'seen result effort aimed connecting mind overseas indian quiz competition know india',\n",
       " 'overseas indian youth hundred country participated competition',\n",
       " 'enthusiasm attraction india encouraging u',\n",
       " 'getting inspiration year organizing quiz even greater scale',\n",
       " 'friend india also get respect contribution development respective country',\n",
       " 'india progress development result increased respect overseas indian community',\n",
       " 'consider overseas indian partner effort development india',\n",
       " 'overseas indian assigned special place action agenda prepared india development till niti aayog',\n",
       " 'brother sister overseas indian several way contributing india developmental journey',\n",
       " 'india largest recipient overseas remittance world',\n",
       " 'indebted every indian living abroad making important contribution india economy',\n",
       " 'one way making investment indian economy',\n",
       " 'today india attractive destination fdi world contribution overseas indian creating awareness facilitating thing immense',\n",
       " 'think given importance people enjoy society play role catalyst thing',\n",
       " 'context diaspora people indian origin immensely contribute promoting tourism',\n",
       " 'friend ceo leader top world company overseas indian',\n",
       " 'fully understand india economy',\n",
       " 'grateful strong confidence india developmental journey',\n",
       " 'today every indian settled abroad considers stake holder india growth',\n",
       " 'want become part change want shoulder responsibility',\n",
       " 'want see rise country global stage',\n",
       " 'aware importance experience bringing social economic change country',\n",
       " 'scheme called vajra mean visiting adjunct joint research faculty started experience help india',\n",
       " 'work indian institution three month scheme',\n",
       " 'today appeal platform associate scheme also encourage indian country get associated',\n",
       " 'also feel good experience benefit young generation india',\n",
       " 'nobody else kind capacity people introduce india requirement india strength uniqueness world',\n",
       " 'value indian civilization culture guide entire world chaotic environment',\n",
       " 'concern health care rise world',\n",
       " 'tell world old tradition holistic living',\n",
       " 'time world community getting divided different ideology different level give example india inclusive philosophy sabka sath sabka vikas development take along everyone',\n",
       " 'concern extremism radicalization rise world reiterate indian culture message amity religion',\n",
       " 'friend aware kumbh fair organised prayag allahabad',\n",
       " 'also matter pride u recently kumbh fair included unesco intangible cultural heritage humanity list',\n",
       " 'uttar pradesh government started preparation comprehensive scale',\n",
       " 'like urge people visit india next year must come prepared pay visit prayag',\n",
       " 'tell people country grand event also become aware legacy indian culture',\n",
       " 'brother sister world facing huge challenge order overcome idea gandhi ji still relevant',\n",
       " 'dispute solved following path non violence passive resistance',\n",
       " 'ideology counter extremism radicalization ideology gandhi ji ideology indian value',\n",
       " 'friend want move forward joining hand order make developed india realising dream new india',\n",
       " 'conference want benefit experience',\n",
       " 'want inform development new india want associate',\n",
       " 'wherever may whichever country might live want become partner developmental journey',\n",
       " 'friend st century called asian century',\n",
       " 'india certainly important role',\n",
       " 'wherever may living feel impact role feel impact rising stature india',\n",
       " 'encouraged work even harder head held high witnessing india economic growth rising power',\n",
       " 'brother sister india country always played positive role global arena',\n",
       " 'never assessed policy towards country scale profit loss saw prism human value',\n",
       " 'model giving developmental aid based give take',\n",
       " 'depends requirement priority recipient country',\n",
       " 'neither intention exploit anybody resource covet country territory',\n",
       " 'focus always capacity building resource development',\n",
       " 'every platform made effort move forward taking everyone along bilateral multilateral platform commonwealth india africa forum summit forum india pacific island cooperation',\n",
       " 'given strong shape already strong relation asean country taking forward relation',\n",
       " 'world able see strong india asean relation day occasion republic day',\n",
       " 'friend india supporter happiness peace prosperity democratic value inclusiveness cooperation brotherhood entire world',\n",
       " 'value connect electorate people representative',\n",
       " 'attempt commitment well india continue contribute peace progress prosperity world',\n",
       " 'friend express heartfelt gratitude people accepting invitation taking time busy schedule attend conference',\n",
       " 'confident conference successful due active participation',\n",
       " 'hope get another opportunity meet people pravasi bhartiya diva next year',\n",
       " 'thank much',\n",
       " 'jai hind',\n",
       " 'akt ak kt rsb akmy best wish occasion pravasi bhartiya day',\n",
       " 'tradition pravasi diva first pravasi parliamentarian conference adding new chapter today',\n",
       " 'extend warm welcome friend come north america south america africa europe asia pacific region also corner world',\n",
       " 'welcome india',\n",
       " 'welcome home ancestor old memory connected different part country',\n",
       " 'ancestor went abroad business education',\n",
       " 'forcibly taken country others taken country using allurement',\n",
       " 'might physically left place left part soul mind land',\n",
       " 'land airport india part soul feel delighted seeing land',\n",
       " 'time throat get choked emotion want express form tear',\n",
       " 'people try best control outpouring emotion succeed',\n",
       " 'eye become moist due tear also lit due visit india',\n",
       " 'understand feeling',\n",
       " 'affection love respect scent air place smell land bow part',\n",
       " 'imagine happy ancestor would seeing today',\n",
       " 'wherever might happiest see land',\n",
       " 'friend india never went heart people went india last hundred year',\n",
       " 'kept alive indian civilization value whichever part world settled',\n",
       " 'surprising wherever people indian origin settled adopted place home fully integrating',\n",
       " 'one hand kept alive indian value within fully integrated language food attire country',\n",
       " 'people indian origin left impression field sport art cinema global platform',\n",
       " 'talk politics see mini world parliament people indian origin eye',\n",
       " 'today people indian origin prime minister mauritius portugal ireland',\n",
       " 'people indian origin occupied office head state head government several country',\n",
       " 'matter great pride u former president guyana shri bharat jagdev ji today present u',\n",
       " 'distinguished people also playing important role politics country',\n",
       " 'friend india motherland ancestor proud',\n",
       " 'achievement success matter pride respect u',\n",
       " 'medium report news people assuming office even news filing nomination people great readership viewership india',\n",
       " 'people affecting geo politics entire region people formulating policy respective country people india read story great interest',\n",
       " 'also discus development like look one u occupied high office',\n",
       " 'people deserve praise giving u happiness making u proud',\n",
       " 'brother sister people living different country long time',\n",
       " 'would experienced last three four year approach india changed',\n",
       " 'greater focus india approach world towards u changing',\n",
       " 'main reason behind thing india changing transforming',\n",
       " 'addition change economic social level change also come level idea well',\n",
       " 'india come long way kind thinking nothing change continue run way nothing going happen',\n",
       " 'time hope expectation indian highest level',\n",
       " 'see complete transformation system see impact irreversible change every sector thing became possible bringing far reaching policy reform every sector indian economy',\n",
       " 'guiding principle reform transform',\n",
       " 'objective make entire system transparent accountable',\n",
       " 'objective completely eradicate corruption',\n",
       " 'friend abolished web hundred tax country gst good service tax integrated country economically',\n",
       " 'single sector like mining fertilizer textile aviation health defense construction real estate food processing brought reform',\n",
       " 'friend today india youngest country world',\n",
       " 'youth unlimited dream expectation',\n",
       " 'government continuously working channelize energy right field business',\n",
       " 'scheme like skill india mission start scheme stand scheme mudra scheme started precisely reason',\n",
       " 'crore loan sanctioned mudra scheme self employment',\n",
       " 'r',\n",
       " 'lakh crore disbursed people without bank guarantee',\n",
       " 'scheme alone provided crore new entrepreneur country',\n",
       " 'government increasing investment infrastructure transport sector keeping requirement india st century mind',\n",
       " 'thing given special importance policy kind logistics india need future',\n",
       " 'highway railway airway waterway port developed manner connected support friend today new railway track india laid twice speed',\n",
       " 'doubling track taking place twice speed',\n",
       " 'national highway built twice speed',\n",
       " 'double new renewable energy capacity connected grid',\n",
       " 'cargo handling growth shipping industry negative earlier government registered growth',\n",
       " 'new employment opportunity generated due effort',\n",
       " 'small scale industry getting new work local level',\n",
       " 'example talk ujjwala scheme limited providing free gas connection poor woman',\n",
       " 'scheme given relief kitchen smoke three crore woman',\n",
       " 'helped making state kerosene free one advantage also',\n",
       " 'new cooking gas dealer appointed number gas delivery men gone launch ujjwala scheme',\n",
       " 'mean addition social reform economic empowerment society also taking place',\n",
       " 'brother sister culture belief whole world family given lot world',\n",
       " 'gone united nation first time put forward proposal international yoga day world',\n",
       " 'aware within day proposal cleared unanimously co sponsored country',\n",
       " 'today way million million people around world celebrate st june yoga day matter pride u',\n",
       " 'method holistic living gift rich tradition india',\n",
       " 'friendsi along president france proposed form international solar alliance time paris agreement climate change',\n",
       " 'become reality',\n",
       " 'making global platform solar technology financing help solar rich country',\n",
       " 'method living maintaining balance nature also indian contribution age',\n",
       " 'brother sister earthquake struck nepal flood sri lanka shortage water maldives india reached first responder',\n",
       " 'crisis yemen safely evacuated citizen also evacuated citizen country',\n",
       " 'protecting human value even time grave crisis part indian tradition considering entire world one family',\n",
       " 'friend hundred year world war',\n",
       " 'lakh indian soldier sacrificed life first second world war',\n",
       " 'india directly involved war',\n",
       " 'india interest single inch land country world war',\n",
       " 'world recognize great sacrifice made india',\n",
       " 'tradition continued even independence',\n",
       " 'india one among nation make maximum contribution un peace keeping force',\n",
       " 'india message sacrifice world human value peace',\n",
       " 'renunciation self interest feeling service renunciation identity',\n",
       " 'india special acceptance world due human value',\n",
       " 'along india society people indian origin people also special acceptance',\n",
       " 'friend whenever visit country always try meet people indian origin living country',\n",
       " 'fortunate meet visit',\n",
       " 'biggest reason behind effort believe permanent ambassador promoting india relation people indian origin',\n",
       " 'constant endeavor regular touch non resident indian continue solve problem',\n",
       " 'earlier separate ministry overseas indian received feedback non resident indian problem coordination ministry external affair',\n",
       " 'merged ministry one getting feedback people',\n",
       " 'would aware earlier two distinct scheme pio oci people even aware distinction two',\n",
       " 'simplified process created one scheme merging',\n",
       " 'minister external affair sushma swaraj ji keep eye problem indian citizen also keep eye people indian origin x find active',\n",
       " 'leadership minister external affair created madad portal real time monitoring extending help consular grievance',\n",
       " 'present pravasi bhartiya day organised every alternate year',\n",
       " 'addition regional pravasi bhartiya day also organised',\n",
       " 'sushma ji come back participating conference singapore',\n",
       " 'brother sister today building present building dedicated people indian origin nd october',\n",
       " 'matter great happiness short span time centre emerged hub people indian origin',\n",
       " 'urge people must visit exhibition life mahatma gandhi organised',\n",
       " 'seen result effort aimed connecting mind overseas indian quiz competition know india',\n",
       " 'overseas indian youth hundred country participated competition',\n",
       " 'enthusiasm attraction india encouraging u',\n",
       " 'getting inspiration year organizing quiz even greater scale',\n",
       " 'friend india also get respect contribution development respective country',\n",
       " 'india progress development result increased respect overseas indian community',\n",
       " 'consider overseas indian partner effort development india',\n",
       " 'overseas indian assigned special place action agenda prepared india development till niti aayog',\n",
       " 'brother sister overseas indian several way contributing india developmental journey',\n",
       " 'india largest recipient overseas remittance world',\n",
       " 'indebted every indian living abroad making important contribution india economy',\n",
       " 'one way making investment indian economy',\n",
       " 'today india attractive destination fdi world contribution overseas indian creating awareness facilitating thing immense',\n",
       " 'think given importance people enjoy society play role catalyst thing',\n",
       " 'context diaspora people indian origin immensely contribute promoting tourism',\n",
       " 'friend ceo leader top world company overseas indian',\n",
       " 'fully understand india economy',\n",
       " 'grateful strong confidence india developmental journey',\n",
       " 'today every indian settled abroad considers stake holder india growth',\n",
       " 'want become part change want shoulder responsibility',\n",
       " 'want see rise country global stage',\n",
       " 'aware importance experience bringing social economic change country',\n",
       " 'scheme called vajra mean visiting adjunct joint research faculty started experience help india',\n",
       " 'work indian institution three month scheme',\n",
       " 'today appeal platform associate scheme also encourage indian country get associated',\n",
       " 'also feel good experience benefit young generation india',\n",
       " 'nobody else kind capacity people introduce india requirement india strength uniqueness world',\n",
       " 'value indian civilization culture guide entire world chaotic environment',\n",
       " 'concern health care rise world',\n",
       " 'tell world old tradition holistic living',\n",
       " 'time world community getting divided different ideology different level give example india inclusive philosophy sabka sath sabka vikas development take along everyone',\n",
       " 'concern extremism radicalization rise world reiterate indian culture message amity religion',\n",
       " 'friend aware kumbh fair organised prayag allahabad',\n",
       " 'also matter pride u recently kumbh fair included unesco intangible cultural heritage humanity list',\n",
       " 'uttar pradesh government started preparation comprehensive scale',\n",
       " 'like urge people visit india next year must come prepared pay visit prayag',\n",
       " 'tell people country grand event also become aware legacy indian culture',\n",
       " 'brother sister world facing huge challenge order overcome idea gandhi ji still relevant',\n",
       " 'dispute solved following path non violence passive resistance',\n",
       " 'ideology counter extremism radicalization ideology gandhi ji ideology indian value',\n",
       " 'friend want move forward joining hand order make developed india realising dream new india',\n",
       " 'conference want benefit experience',\n",
       " 'want inform development new india want associate',\n",
       " 'wherever may whichever country might live want become partner developmental journey',\n",
       " 'friend st century called asian century',\n",
       " 'india certainly important role',\n",
       " 'wherever may living feel impact role feel impact rising stature india',\n",
       " 'encouraged work even harder head held high witnessing india economic growth rising power',\n",
       " 'brother sister india country always played positive role global arena',\n",
       " 'never assessed policy towards country scale profit loss saw prism human value',\n",
       " 'model giving developmental aid based give take',\n",
       " 'depends requirement priority recipient country',\n",
       " 'neither intention exploit anybody resource covet country territory',\n",
       " 'focus always capacity building resource development',\n",
       " 'every platform made effort move forward taking everyone along bilateral multilateral platform commonwealth india africa forum summit forum india pacific island cooperation',\n",
       " 'given strong shape already strong relation asean country taking forward relation',\n",
       " 'world able see strong india asean relation day occasion republic day',\n",
       " 'friend india supporter happiness peace prosperity democratic value inclusiveness cooperation brotherhood entire world',\n",
       " 'value connect electorate people representative',\n",
       " 'attempt commitment well india continue contribute peace progress prosperity world',\n",
       " 'friend express heartfelt gratitude people accepting invitation taking time busy schedule attend conference',\n",
       " 'confident conference successful due active participation',\n",
       " 'hope get another opportunity meet people pravasi bhartiya diva next year',\n",
       " 'thank much',\n",
       " 'jai hind',\n",
       " 'akt ak kt rsb ak']"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_data['Cleaned_Speech'][100] # Remove the last element from this/ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1182"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_data.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "matrix = []\n",
    "for i in range(df_data.shape[0]):\n",
    "    matrix.append(df_data['Vectorized_Speech'][i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "np_m = np.array(matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>290</th>\n",
       "      <th>291</th>\n",
       "      <th>292</th>\n",
       "      <th>293</th>\n",
       "      <th>294</th>\n",
       "      <th>295</th>\n",
       "      <th>296</th>\n",
       "      <th>297</th>\n",
       "      <th>298</th>\n",
       "      <th>299</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.000055</td>\n",
       "      <td>0.000329</td>\n",
       "      <td>0.000114</td>\n",
       "      <td>0.000119</td>\n",
       "      <td>-0.000108</td>\n",
       "      <td>-0.000408</td>\n",
       "      <td>0.000313</td>\n",
       "      <td>0.000755</td>\n",
       "      <td>0.000032</td>\n",
       "      <td>-0.000158</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.000012</td>\n",
       "      <td>0.000391</td>\n",
       "      <td>0.000274</td>\n",
       "      <td>-0.000001</td>\n",
       "      <td>0.000454</td>\n",
       "      <td>0.000449</td>\n",
       "      <td>0.000120</td>\n",
       "      <td>-0.000147</td>\n",
       "      <td>0.000272</td>\n",
       "      <td>-0.000329</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.000020</td>\n",
       "      <td>0.000442</td>\n",
       "      <td>0.000112</td>\n",
       "      <td>0.000208</td>\n",
       "      <td>0.000005</td>\n",
       "      <td>-0.000525</td>\n",
       "      <td>0.000422</td>\n",
       "      <td>0.001080</td>\n",
       "      <td>-0.000090</td>\n",
       "      <td>-0.000192</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000161</td>\n",
       "      <td>0.000536</td>\n",
       "      <td>0.000385</td>\n",
       "      <td>-0.000027</td>\n",
       "      <td>0.000712</td>\n",
       "      <td>0.000668</td>\n",
       "      <td>0.000166</td>\n",
       "      <td>-0.000313</td>\n",
       "      <td>0.000258</td>\n",
       "      <td>-0.000369</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.000026</td>\n",
       "      <td>0.000505</td>\n",
       "      <td>-0.000108</td>\n",
       "      <td>0.000167</td>\n",
       "      <td>-0.000011</td>\n",
       "      <td>-0.000569</td>\n",
       "      <td>0.000331</td>\n",
       "      <td>0.000908</td>\n",
       "      <td>0.000137</td>\n",
       "      <td>-0.000315</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000006</td>\n",
       "      <td>0.000686</td>\n",
       "      <td>0.000301</td>\n",
       "      <td>-0.000062</td>\n",
       "      <td>0.000543</td>\n",
       "      <td>0.000712</td>\n",
       "      <td>-0.000019</td>\n",
       "      <td>-0.000205</td>\n",
       "      <td>0.000307</td>\n",
       "      <td>-0.000304</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.000012</td>\n",
       "      <td>0.000445</td>\n",
       "      <td>0.000062</td>\n",
       "      <td>0.000202</td>\n",
       "      <td>0.000027</td>\n",
       "      <td>-0.000410</td>\n",
       "      <td>0.000272</td>\n",
       "      <td>0.000806</td>\n",
       "      <td>0.000136</td>\n",
       "      <td>-0.000138</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000021</td>\n",
       "      <td>0.000378</td>\n",
       "      <td>0.000319</td>\n",
       "      <td>0.000064</td>\n",
       "      <td>0.000324</td>\n",
       "      <td>0.000479</td>\n",
       "      <td>-0.000025</td>\n",
       "      <td>-0.000281</td>\n",
       "      <td>0.000294</td>\n",
       "      <td>-0.000148</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.000045</td>\n",
       "      <td>0.000299</td>\n",
       "      <td>-0.000050</td>\n",
       "      <td>0.000097</td>\n",
       "      <td>-0.000038</td>\n",
       "      <td>-0.000325</td>\n",
       "      <td>0.000155</td>\n",
       "      <td>0.000604</td>\n",
       "      <td>0.000111</td>\n",
       "      <td>-0.000187</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000020</td>\n",
       "      <td>0.000328</td>\n",
       "      <td>0.000179</td>\n",
       "      <td>-0.000012</td>\n",
       "      <td>0.000353</td>\n",
       "      <td>0.000362</td>\n",
       "      <td>-0.000007</td>\n",
       "      <td>-0.000128</td>\n",
       "      <td>0.000128</td>\n",
       "      <td>-0.000127</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1177</th>\n",
       "      <td>-0.000030</td>\n",
       "      <td>0.000492</td>\n",
       "      <td>0.000086</td>\n",
       "      <td>0.000339</td>\n",
       "      <td>-0.000139</td>\n",
       "      <td>-0.000608</td>\n",
       "      <td>0.000412</td>\n",
       "      <td>0.001221</td>\n",
       "      <td>0.000014</td>\n",
       "      <td>-0.000419</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000115</td>\n",
       "      <td>0.000690</td>\n",
       "      <td>0.000379</td>\n",
       "      <td>-0.000022</td>\n",
       "      <td>0.000653</td>\n",
       "      <td>0.000765</td>\n",
       "      <td>0.000104</td>\n",
       "      <td>-0.000259</td>\n",
       "      <td>0.000380</td>\n",
       "      <td>-0.000400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1178</th>\n",
       "      <td>0.000045</td>\n",
       "      <td>0.000287</td>\n",
       "      <td>0.000016</td>\n",
       "      <td>0.000176</td>\n",
       "      <td>0.000065</td>\n",
       "      <td>-0.000276</td>\n",
       "      <td>0.000197</td>\n",
       "      <td>0.000679</td>\n",
       "      <td>-0.000118</td>\n",
       "      <td>-0.000070</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000083</td>\n",
       "      <td>0.000295</td>\n",
       "      <td>0.000242</td>\n",
       "      <td>-0.000094</td>\n",
       "      <td>0.000360</td>\n",
       "      <td>0.000451</td>\n",
       "      <td>0.000098</td>\n",
       "      <td>-0.000203</td>\n",
       "      <td>0.000176</td>\n",
       "      <td>-0.000201</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1179</th>\n",
       "      <td>0.000030</td>\n",
       "      <td>0.000334</td>\n",
       "      <td>0.000005</td>\n",
       "      <td>0.000187</td>\n",
       "      <td>-0.000125</td>\n",
       "      <td>-0.000368</td>\n",
       "      <td>0.000311</td>\n",
       "      <td>0.000792</td>\n",
       "      <td>-0.000078</td>\n",
       "      <td>-0.000285</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000108</td>\n",
       "      <td>0.000422</td>\n",
       "      <td>0.000147</td>\n",
       "      <td>-0.000026</td>\n",
       "      <td>0.000444</td>\n",
       "      <td>0.000437</td>\n",
       "      <td>0.000049</td>\n",
       "      <td>-0.000237</td>\n",
       "      <td>0.000317</td>\n",
       "      <td>-0.000332</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1180</th>\n",
       "      <td>-0.000045</td>\n",
       "      <td>0.000286</td>\n",
       "      <td>0.000013</td>\n",
       "      <td>0.000128</td>\n",
       "      <td>-0.000006</td>\n",
       "      <td>-0.000425</td>\n",
       "      <td>0.000185</td>\n",
       "      <td>0.000757</td>\n",
       "      <td>0.000114</td>\n",
       "      <td>-0.000169</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000114</td>\n",
       "      <td>0.000419</td>\n",
       "      <td>0.000290</td>\n",
       "      <td>0.000027</td>\n",
       "      <td>0.000461</td>\n",
       "      <td>0.000527</td>\n",
       "      <td>-0.000023</td>\n",
       "      <td>-0.000118</td>\n",
       "      <td>0.000197</td>\n",
       "      <td>-0.000200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1181</th>\n",
       "      <td>-0.000012</td>\n",
       "      <td>0.000420</td>\n",
       "      <td>-0.000038</td>\n",
       "      <td>0.000217</td>\n",
       "      <td>-0.000039</td>\n",
       "      <td>-0.000521</td>\n",
       "      <td>0.000308</td>\n",
       "      <td>0.001030</td>\n",
       "      <td>0.000142</td>\n",
       "      <td>-0.000321</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000078</td>\n",
       "      <td>0.000505</td>\n",
       "      <td>0.000228</td>\n",
       "      <td>-0.000017</td>\n",
       "      <td>0.000646</td>\n",
       "      <td>0.000622</td>\n",
       "      <td>-0.000016</td>\n",
       "      <td>-0.000208</td>\n",
       "      <td>0.000205</td>\n",
       "      <td>-0.000299</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1182 rows × 300 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           0         1         2         3         4         5         6    \\\n",
       "0     0.000055  0.000329  0.000114  0.000119 -0.000108 -0.000408  0.000313   \n",
       "1    -0.000020  0.000442  0.000112  0.000208  0.000005 -0.000525  0.000422   \n",
       "2    -0.000026  0.000505 -0.000108  0.000167 -0.000011 -0.000569  0.000331   \n",
       "3    -0.000012  0.000445  0.000062  0.000202  0.000027 -0.000410  0.000272   \n",
       "4    -0.000045  0.000299 -0.000050  0.000097 -0.000038 -0.000325  0.000155   \n",
       "...        ...       ...       ...       ...       ...       ...       ...   \n",
       "1177 -0.000030  0.000492  0.000086  0.000339 -0.000139 -0.000608  0.000412   \n",
       "1178  0.000045  0.000287  0.000016  0.000176  0.000065 -0.000276  0.000197   \n",
       "1179  0.000030  0.000334  0.000005  0.000187 -0.000125 -0.000368  0.000311   \n",
       "1180 -0.000045  0.000286  0.000013  0.000128 -0.000006 -0.000425  0.000185   \n",
       "1181 -0.000012  0.000420 -0.000038  0.000217 -0.000039 -0.000521  0.000308   \n",
       "\n",
       "           7         8         9    ...       290       291       292  \\\n",
       "0     0.000755  0.000032 -0.000158  ... -0.000012  0.000391  0.000274   \n",
       "1     0.001080 -0.000090 -0.000192  ...  0.000161  0.000536  0.000385   \n",
       "2     0.000908  0.000137 -0.000315  ...  0.000006  0.000686  0.000301   \n",
       "3     0.000806  0.000136 -0.000138  ...  0.000021  0.000378  0.000319   \n",
       "4     0.000604  0.000111 -0.000187  ...  0.000020  0.000328  0.000179   \n",
       "...        ...       ...       ...  ...       ...       ...       ...   \n",
       "1177  0.001221  0.000014 -0.000419  ...  0.000115  0.000690  0.000379   \n",
       "1178  0.000679 -0.000118 -0.000070  ...  0.000083  0.000295  0.000242   \n",
       "1179  0.000792 -0.000078 -0.000285  ...  0.000108  0.000422  0.000147   \n",
       "1180  0.000757  0.000114 -0.000169  ...  0.000114  0.000419  0.000290   \n",
       "1181  0.001030  0.000142 -0.000321  ...  0.000078  0.000505  0.000228   \n",
       "\n",
       "           293       294       295       296       297       298       299  \n",
       "0    -0.000001  0.000454  0.000449  0.000120 -0.000147  0.000272 -0.000329  \n",
       "1    -0.000027  0.000712  0.000668  0.000166 -0.000313  0.000258 -0.000369  \n",
       "2    -0.000062  0.000543  0.000712 -0.000019 -0.000205  0.000307 -0.000304  \n",
       "3     0.000064  0.000324  0.000479 -0.000025 -0.000281  0.000294 -0.000148  \n",
       "4    -0.000012  0.000353  0.000362 -0.000007 -0.000128  0.000128 -0.000127  \n",
       "...        ...       ...       ...       ...       ...       ...       ...  \n",
       "1177 -0.000022  0.000653  0.000765  0.000104 -0.000259  0.000380 -0.000400  \n",
       "1178 -0.000094  0.000360  0.000451  0.000098 -0.000203  0.000176 -0.000201  \n",
       "1179 -0.000026  0.000444  0.000437  0.000049 -0.000237  0.000317 -0.000332  \n",
       "1180  0.000027  0.000461  0.000527 -0.000023 -0.000118  0.000197 -0.000200  \n",
       "1181 -0.000017  0.000646  0.000622 -0.000016 -0.000208  0.000205 -0.000299  \n",
       "\n",
       "[1182 rows x 300 columns]"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np_m_df = pd.DataFrame(np_m)\n",
    "np_m_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>290</th>\n",
       "      <th>291</th>\n",
       "      <th>292</th>\n",
       "      <th>293</th>\n",
       "      <th>294</th>\n",
       "      <th>295</th>\n",
       "      <th>296</th>\n",
       "      <th>297</th>\n",
       "      <th>298</th>\n",
       "      <th>299</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.879742</td>\n",
       "      <td>0.085635</td>\n",
       "      <td>0.280214</td>\n",
       "      <td>0.149869</td>\n",
       "      <td>0.410721</td>\n",
       "      <td>0.964471</td>\n",
       "      <td>0.049500</td>\n",
       "      <td>0.020517</td>\n",
       "      <td>0.227270</td>\n",
       "      <td>0.895903</td>\n",
       "      <td>...</td>\n",
       "      <td>0.498700</td>\n",
       "      <td>0.027320</td>\n",
       "      <td>0.049409</td>\n",
       "      <td>0.581606</td>\n",
       "      <td>0.049421</td>\n",
       "      <td>0.030612</td>\n",
       "      <td>0.551777</td>\n",
       "      <td>0.901156</td>\n",
       "      <td>0.069885</td>\n",
       "      <td>0.832883</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.836504</td>\n",
       "      <td>0.106268</td>\n",
       "      <td>0.279186</td>\n",
       "      <td>0.189869</td>\n",
       "      <td>0.477076</td>\n",
       "      <td>0.941234</td>\n",
       "      <td>0.079705</td>\n",
       "      <td>0.055203</td>\n",
       "      <td>0.162551</td>\n",
       "      <td>0.879453</td>\n",
       "      <td>...</td>\n",
       "      <td>0.610892</td>\n",
       "      <td>0.057937</td>\n",
       "      <td>0.075616</td>\n",
       "      <td>0.569215</td>\n",
       "      <td>0.124347</td>\n",
       "      <td>0.073970</td>\n",
       "      <td>0.582357</td>\n",
       "      <td>0.856149</td>\n",
       "      <td>0.065231</td>\n",
       "      <td>0.815615</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.832703</td>\n",
       "      <td>0.117942</td>\n",
       "      <td>0.111305</td>\n",
       "      <td>0.171239</td>\n",
       "      <td>0.467508</td>\n",
       "      <td>0.932522</td>\n",
       "      <td>0.054544</td>\n",
       "      <td>0.036838</td>\n",
       "      <td>0.282884</td>\n",
       "      <td>0.820145</td>\n",
       "      <td>...</td>\n",
       "      <td>0.510437</td>\n",
       "      <td>0.089846</td>\n",
       "      <td>0.055937</td>\n",
       "      <td>0.551652</td>\n",
       "      <td>0.075254</td>\n",
       "      <td>0.082660</td>\n",
       "      <td>0.458407</td>\n",
       "      <td>0.885343</td>\n",
       "      <td>0.081385</td>\n",
       "      <td>0.843604</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.840986</td>\n",
       "      <td>0.106825</td>\n",
       "      <td>0.240669</td>\n",
       "      <td>0.187034</td>\n",
       "      <td>0.490078</td>\n",
       "      <td>0.964111</td>\n",
       "      <td>0.038195</td>\n",
       "      <td>0.026015</td>\n",
       "      <td>0.281973</td>\n",
       "      <td>0.905181</td>\n",
       "      <td>...</td>\n",
       "      <td>0.519874</td>\n",
       "      <td>0.024470</td>\n",
       "      <td>0.060213</td>\n",
       "      <td>0.613723</td>\n",
       "      <td>0.011912</td>\n",
       "      <td>0.036528</td>\n",
       "      <td>0.454286</td>\n",
       "      <td>0.864814</td>\n",
       "      <td>0.077277</td>\n",
       "      <td>0.910023</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.821982</td>\n",
       "      <td>0.080235</td>\n",
       "      <td>0.155331</td>\n",
       "      <td>0.139735</td>\n",
       "      <td>0.451723</td>\n",
       "      <td>0.980944</td>\n",
       "      <td>0.005654</td>\n",
       "      <td>0.004388</td>\n",
       "      <td>0.269059</td>\n",
       "      <td>0.881707</td>\n",
       "      <td>...</td>\n",
       "      <td>0.519372</td>\n",
       "      <td>0.013811</td>\n",
       "      <td>0.027030</td>\n",
       "      <td>0.576257</td>\n",
       "      <td>0.020236</td>\n",
       "      <td>0.013313</td>\n",
       "      <td>0.466610</td>\n",
       "      <td>0.906133</td>\n",
       "      <td>0.022629</td>\n",
       "      <td>0.918730</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1177</th>\n",
       "      <td>0.830493</td>\n",
       "      <td>0.115558</td>\n",
       "      <td>0.259110</td>\n",
       "      <td>0.248762</td>\n",
       "      <td>0.392465</td>\n",
       "      <td>0.924909</td>\n",
       "      <td>0.076717</td>\n",
       "      <td>0.070264</td>\n",
       "      <td>0.217646</td>\n",
       "      <td>0.770051</td>\n",
       "      <td>...</td>\n",
       "      <td>0.580971</td>\n",
       "      <td>0.090674</td>\n",
       "      <td>0.074362</td>\n",
       "      <td>0.571345</td>\n",
       "      <td>0.107182</td>\n",
       "      <td>0.093079</td>\n",
       "      <td>0.540789</td>\n",
       "      <td>0.870597</td>\n",
       "      <td>0.105248</td>\n",
       "      <td>0.802517</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1178</th>\n",
       "      <td>0.873879</td>\n",
       "      <td>0.078034</td>\n",
       "      <td>0.205670</td>\n",
       "      <td>0.175532</td>\n",
       "      <td>0.512358</td>\n",
       "      <td>0.990566</td>\n",
       "      <td>0.017361</td>\n",
       "      <td>0.012375</td>\n",
       "      <td>0.147338</td>\n",
       "      <td>0.938298</td>\n",
       "      <td>...</td>\n",
       "      <td>0.560101</td>\n",
       "      <td>0.006802</td>\n",
       "      <td>0.041853</td>\n",
       "      <td>0.535858</td>\n",
       "      <td>0.022217</td>\n",
       "      <td>0.030894</td>\n",
       "      <td>0.537090</td>\n",
       "      <td>0.885977</td>\n",
       "      <td>0.038622</td>\n",
       "      <td>0.887289</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1179</th>\n",
       "      <td>0.865288</td>\n",
       "      <td>0.086543</td>\n",
       "      <td>0.197648</td>\n",
       "      <td>0.180101</td>\n",
       "      <td>0.400704</td>\n",
       "      <td>0.972449</td>\n",
       "      <td>0.048958</td>\n",
       "      <td>0.024447</td>\n",
       "      <td>0.168668</td>\n",
       "      <td>0.834417</td>\n",
       "      <td>...</td>\n",
       "      <td>0.576484</td>\n",
       "      <td>0.033808</td>\n",
       "      <td>0.019456</td>\n",
       "      <td>0.569702</td>\n",
       "      <td>0.046682</td>\n",
       "      <td>0.028150</td>\n",
       "      <td>0.504341</td>\n",
       "      <td>0.876764</td>\n",
       "      <td>0.084780</td>\n",
       "      <td>0.831694</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1180</th>\n",
       "      <td>0.821766</td>\n",
       "      <td>0.077706</td>\n",
       "      <td>0.203304</td>\n",
       "      <td>0.153766</td>\n",
       "      <td>0.470526</td>\n",
       "      <td>0.961035</td>\n",
       "      <td>0.014116</td>\n",
       "      <td>0.020693</td>\n",
       "      <td>0.270386</td>\n",
       "      <td>0.890261</td>\n",
       "      <td>...</td>\n",
       "      <td>0.580202</td>\n",
       "      <td>0.033275</td>\n",
       "      <td>0.053359</td>\n",
       "      <td>0.595481</td>\n",
       "      <td>0.051605</td>\n",
       "      <td>0.046055</td>\n",
       "      <td>0.455575</td>\n",
       "      <td>0.908770</td>\n",
       "      <td>0.045537</td>\n",
       "      <td>0.887658</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1181</th>\n",
       "      <td>0.840699</td>\n",
       "      <td>0.102317</td>\n",
       "      <td>0.164311</td>\n",
       "      <td>0.194024</td>\n",
       "      <td>0.451425</td>\n",
       "      <td>0.942039</td>\n",
       "      <td>0.048122</td>\n",
       "      <td>0.049909</td>\n",
       "      <td>0.285491</td>\n",
       "      <td>0.817116</td>\n",
       "      <td>...</td>\n",
       "      <td>0.557091</td>\n",
       "      <td>0.051325</td>\n",
       "      <td>0.038581</td>\n",
       "      <td>0.573892</td>\n",
       "      <td>0.105155</td>\n",
       "      <td>0.064789</td>\n",
       "      <td>0.460726</td>\n",
       "      <td>0.884597</td>\n",
       "      <td>0.047987</td>\n",
       "      <td>0.845468</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1182 rows × 300 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           0         1         2         3         4         5         6    \\\n",
       "0     0.879742  0.085635  0.280214  0.149869  0.410721  0.964471  0.049500   \n",
       "1     0.836504  0.106268  0.279186  0.189869  0.477076  0.941234  0.079705   \n",
       "2     0.832703  0.117942  0.111305  0.171239  0.467508  0.932522  0.054544   \n",
       "3     0.840986  0.106825  0.240669  0.187034  0.490078  0.964111  0.038195   \n",
       "4     0.821982  0.080235  0.155331  0.139735  0.451723  0.980944  0.005654   \n",
       "...        ...       ...       ...       ...       ...       ...       ...   \n",
       "1177  0.830493  0.115558  0.259110  0.248762  0.392465  0.924909  0.076717   \n",
       "1178  0.873879  0.078034  0.205670  0.175532  0.512358  0.990566  0.017361   \n",
       "1179  0.865288  0.086543  0.197648  0.180101  0.400704  0.972449  0.048958   \n",
       "1180  0.821766  0.077706  0.203304  0.153766  0.470526  0.961035  0.014116   \n",
       "1181  0.840699  0.102317  0.164311  0.194024  0.451425  0.942039  0.048122   \n",
       "\n",
       "           7         8         9    ...       290       291       292  \\\n",
       "0     0.020517  0.227270  0.895903  ...  0.498700  0.027320  0.049409   \n",
       "1     0.055203  0.162551  0.879453  ...  0.610892  0.057937  0.075616   \n",
       "2     0.036838  0.282884  0.820145  ...  0.510437  0.089846  0.055937   \n",
       "3     0.026015  0.281973  0.905181  ...  0.519874  0.024470  0.060213   \n",
       "4     0.004388  0.269059  0.881707  ...  0.519372  0.013811  0.027030   \n",
       "...        ...       ...       ...  ...       ...       ...       ...   \n",
       "1177  0.070264  0.217646  0.770051  ...  0.580971  0.090674  0.074362   \n",
       "1178  0.012375  0.147338  0.938298  ...  0.560101  0.006802  0.041853   \n",
       "1179  0.024447  0.168668  0.834417  ...  0.576484  0.033808  0.019456   \n",
       "1180  0.020693  0.270386  0.890261  ...  0.580202  0.033275  0.053359   \n",
       "1181  0.049909  0.285491  0.817116  ...  0.557091  0.051325  0.038581   \n",
       "\n",
       "           293       294       295       296       297       298       299  \n",
       "0     0.581606  0.049421  0.030612  0.551777  0.901156  0.069885  0.832883  \n",
       "1     0.569215  0.124347  0.073970  0.582357  0.856149  0.065231  0.815615  \n",
       "2     0.551652  0.075254  0.082660  0.458407  0.885343  0.081385  0.843604  \n",
       "3     0.613723  0.011912  0.036528  0.454286  0.864814  0.077277  0.910023  \n",
       "4     0.576257  0.020236  0.013313  0.466610  0.906133  0.022629  0.918730  \n",
       "...        ...       ...       ...       ...       ...       ...       ...  \n",
       "1177  0.571345  0.107182  0.093079  0.540789  0.870597  0.105248  0.802517  \n",
       "1178  0.535858  0.022217  0.030894  0.537090  0.885977  0.038622  0.887289  \n",
       "1179  0.569702  0.046682  0.028150  0.504341  0.876764  0.084780  0.831694  \n",
       "1180  0.595481  0.051605  0.046055  0.455575  0.908770  0.045537  0.887658  \n",
       "1181  0.573892  0.105155  0.064789  0.460726  0.884597  0.047987  0.845468  \n",
       "\n",
       "[1182 rows x 300 columns]"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Min Max Scaling\n",
    "np_m_df_norm = (np_m_df-np_m_df.min())/(np_m_df.max()-np_m_df.min())\n",
    "np_m_df_norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "353510"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(np.unique(np_m))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_data.to_csv(\"E:\\Tarang\\Ashoka\\Python\\PYTHON PROJECT\\PM_Modi_Speech_Cleaned.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(speech):\n",
    "    # Sentence Tokenization\n",
    "    sentence = nltk.sent_tokenize(speech)\n",
    "\n",
    "    # Creating an empty list to store the cleaned dataset \n",
    "    corpus = []\n",
    "\n",
    "    # Loop through each sentence in the speech\n",
    "    for i in range(len(sentence)):\n",
    "        # Remove any non-alphabetic characters in the sentence and convert to lowercase\n",
    "        review = re.sub(\"[^a-zA-Z]\", \" \", sentence[i])\n",
    "        review = review.lower()\n",
    "        \n",
    "        # Tokenize the sentence into a list of words\n",
    "        review = review.split()\n",
    "        \n",
    "        # Lemmatize each word in the sentence and remove stop words\n",
    "        lemmatizer = WordNetLemmatizer()\n",
    "        review = [lemmatizer.lemmatize(word) for word in review if word not in set(stopwords.words(\"english\"))]\n",
    "        \n",
    "        # Convert the list of words back into a sentence\n",
    "        review = ' '.join(review)\n",
    "        \n",
    "        # Add the cleaned sentence to the corpus\n",
    "        corpus.append(review)\n",
    "\n",
    "    # Tokenize each sentence in the corpus into a list of words\n",
    "    corpus_tokenized = [nltk.word_tokenize(sentence) for sentence in corpus]\n",
    "\n",
    "    # Build the vocabulary for the Word2Vec model\n",
    "    model = Word2Vec(vector_size=300, window=5, min_count=1, workers=4)\n",
    "    model.build_vocab(sentences=corpus_tokenized)\n",
    "\n",
    "    # Train the Word2Vec model on the tokenized corpus\n",
    "    #model.train(sentences=corpus_tokenized, total_examples=len(corpus_tokenized), epochs=100)\n",
    "\n",
    "    # Extract the word vectors for each word in the corpus\n",
    "    vectors = []\n",
    "    for sentence in corpus_tokenized:\n",
    "        for word in sentence:\n",
    "            if word in model.wv.key_to_index:\n",
    "                # If the word is in the Word2Vec model's vocabulary, extract its vector\n",
    "                word_vector = model.wv.get_vector(word)\n",
    "                vectors.append(word_vector)\n",
    "            else:\n",
    "                # If the word is not in the vocabulary, print a message to the console\n",
    "                print(f\"{word} not in vocabulary.\")\n",
    "    \n",
    "    # Calculate the average vector for the entire speech\n",
    "    vectors_array = np.array(vectors)\n",
    "    avg_vector = np.mean(vectors_array, axis=0)\n",
    "    \n",
    "    return avg_vector\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "Either one of corpus_file or corpus_iterable value must be provided",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[20], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[39m# Vectorize every speech \u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m df_data[\u001b[39m'\u001b[39m\u001b[39mVectorized_Speech\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m=\u001b[39m df_data[\u001b[39m'\u001b[39;49m\u001b[39mSpeech\u001b[39;49m\u001b[39m'\u001b[39;49m]\u001b[39m.\u001b[39;49mapply(tokenize)\n",
      "File \u001b[1;32mc:\\Users\\hp\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pandas\\core\\series.py:4771\u001b[0m, in \u001b[0;36mSeries.apply\u001b[1;34m(self, func, convert_dtype, args, **kwargs)\u001b[0m\n\u001b[0;32m   4661\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mapply\u001b[39m(\n\u001b[0;32m   4662\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[0;32m   4663\u001b[0m     func: AggFuncType,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   4666\u001b[0m     \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs,\n\u001b[0;32m   4667\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m DataFrame \u001b[39m|\u001b[39m Series:\n\u001b[0;32m   4668\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m   4669\u001b[0m \u001b[39m    Invoke function on values of Series.\u001b[39;00m\n\u001b[0;32m   4670\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   4769\u001b[0m \u001b[39m    dtype: float64\u001b[39;00m\n\u001b[0;32m   4770\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m-> 4771\u001b[0m     \u001b[39mreturn\u001b[39;00m SeriesApply(\u001b[39mself\u001b[39;49m, func, convert_dtype, args, kwargs)\u001b[39m.\u001b[39;49mapply()\n",
      "File \u001b[1;32mc:\\Users\\hp\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pandas\\core\\apply.py:1123\u001b[0m, in \u001b[0;36mSeriesApply.apply\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1120\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mapply_str()\n\u001b[0;32m   1122\u001b[0m \u001b[39m# self.f is Callable\u001b[39;00m\n\u001b[1;32m-> 1123\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mapply_standard()\n",
      "File \u001b[1;32mc:\\Users\\hp\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pandas\\core\\apply.py:1174\u001b[0m, in \u001b[0;36mSeriesApply.apply_standard\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1172\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m   1173\u001b[0m         values \u001b[39m=\u001b[39m obj\u001b[39m.\u001b[39mastype(\u001b[39mobject\u001b[39m)\u001b[39m.\u001b[39m_values\n\u001b[1;32m-> 1174\u001b[0m         mapped \u001b[39m=\u001b[39m lib\u001b[39m.\u001b[39;49mmap_infer(\n\u001b[0;32m   1175\u001b[0m             values,\n\u001b[0;32m   1176\u001b[0m             f,\n\u001b[0;32m   1177\u001b[0m             convert\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mconvert_dtype,\n\u001b[0;32m   1178\u001b[0m         )\n\u001b[0;32m   1180\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(mapped) \u001b[39mand\u001b[39;00m \u001b[39misinstance\u001b[39m(mapped[\u001b[39m0\u001b[39m], ABCSeries):\n\u001b[0;32m   1181\u001b[0m     \u001b[39m# GH#43986 Need to do list(mapped) in order to get treated as nested\u001b[39;00m\n\u001b[0;32m   1182\u001b[0m     \u001b[39m#  See also GH#25959 regarding EA support\u001b[39;00m\n\u001b[0;32m   1183\u001b[0m     \u001b[39mreturn\u001b[39;00m obj\u001b[39m.\u001b[39m_constructor_expanddim(\u001b[39mlist\u001b[39m(mapped), index\u001b[39m=\u001b[39mobj\u001b[39m.\u001b[39mindex)\n",
      "File \u001b[1;32mc:\\Users\\hp\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pandas\\_libs\\lib.pyx:2924\u001b[0m, in \u001b[0;36mpandas._libs.lib.map_infer\u001b[1;34m()\u001b[0m\n",
      "Cell \u001b[1;32mIn[17], line 32\u001b[0m, in \u001b[0;36mtokenize\u001b[1;34m(speech)\u001b[0m\n\u001b[0;32m     30\u001b[0m \u001b[39m# Build the vocabulary for the Word2Vec model\u001b[39;00m\n\u001b[0;32m     31\u001b[0m model \u001b[39m=\u001b[39m Word2Vec(vector_size\u001b[39m=\u001b[39m\u001b[39m300\u001b[39m, window\u001b[39m=\u001b[39m\u001b[39m5\u001b[39m, min_count\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m, workers\u001b[39m=\u001b[39m\u001b[39m4\u001b[39m)\n\u001b[1;32m---> 32\u001b[0m model\u001b[39m.\u001b[39;49mbuild_vocab(sentences\u001b[39m=\u001b[39;49mcorpus_tokenized)\n\u001b[0;32m     34\u001b[0m \u001b[39m# Train the Word2Vec model on the tokenized corpus\u001b[39;00m\n\u001b[0;32m     35\u001b[0m \u001b[39m#model.train(sentences=corpus_tokenized, total_examples=len(corpus_tokenized), epochs=100)\u001b[39;00m\n\u001b[0;32m     36\u001b[0m \n\u001b[0;32m     37\u001b[0m \u001b[39m# Extract the word vectors for each word in the corpus\u001b[39;00m\n\u001b[0;32m     38\u001b[0m vectors \u001b[39m=\u001b[39m []\n",
      "File \u001b[1;32mc:\\Users\\hp\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\gensim\\models\\word2vec.py:490\u001b[0m, in \u001b[0;36mWord2Vec.build_vocab\u001b[1;34m(self, corpus_iterable, corpus_file, update, progress_per, keep_raw_vocab, trim_rule, **kwargs)\u001b[0m\n\u001b[0;32m    449\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mbuild_vocab\u001b[39m(\n\u001b[0;32m    450\u001b[0m         \u001b[39mself\u001b[39m, corpus_iterable\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, corpus_file\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, update\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m, progress_per\u001b[39m=\u001b[39m\u001b[39m10000\u001b[39m,\n\u001b[0;32m    451\u001b[0m         keep_raw_vocab\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m, trim_rule\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs,\n\u001b[0;32m    452\u001b[0m     ):\n\u001b[0;32m    453\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Build vocabulary from a sequence of sentences (can be a once-only generator stream).\u001b[39;00m\n\u001b[0;32m    454\u001b[0m \n\u001b[0;32m    455\u001b[0m \u001b[39m    Parameters\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    488\u001b[0m \n\u001b[0;32m    489\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 490\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_check_corpus_sanity(corpus_iterable\u001b[39m=\u001b[39;49mcorpus_iterable, corpus_file\u001b[39m=\u001b[39;49mcorpus_file, passes\u001b[39m=\u001b[39;49m\u001b[39m1\u001b[39;49m)\n\u001b[0;32m    491\u001b[0m     total_words, corpus_count \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mscan_vocab(\n\u001b[0;32m    492\u001b[0m         corpus_iterable\u001b[39m=\u001b[39mcorpus_iterable, corpus_file\u001b[39m=\u001b[39mcorpus_file, progress_per\u001b[39m=\u001b[39mprogress_per, trim_rule\u001b[39m=\u001b[39mtrim_rule)\n\u001b[0;32m    493\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcorpus_count \u001b[39m=\u001b[39m corpus_count\n",
      "File \u001b[1;32mc:\\Users\\hp\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\gensim\\models\\word2vec.py:1497\u001b[0m, in \u001b[0;36mWord2Vec._check_corpus_sanity\u001b[1;34m(self, corpus_iterable, corpus_file, passes)\u001b[0m\n\u001b[0;32m   1495\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"Checks whether the corpus parameters make sense.\"\"\"\u001b[39;00m\n\u001b[0;32m   1496\u001b[0m \u001b[39mif\u001b[39;00m corpus_file \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m corpus_iterable \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m-> 1497\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mTypeError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mEither one of corpus_file or corpus_iterable value must be provided\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m corpus_file \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m corpus_iterable \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m   1499\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mTypeError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mBoth corpus_file and corpus_iterable must not be provided at the same time\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "\u001b[1;31mTypeError\u001b[0m: Either one of corpus_file or corpus_iterable value must be provided"
     ]
    }
   ],
   "source": [
    "\n",
    "# Vectorize every speech \n",
    "df_data['Vectorized_Speech'] = df_data['Speech'].apply(tokenize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\hp\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\hp\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\hp\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "you must first build vocabulary before training the model",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[21], line 62\u001b[0m\n\u001b[0;32m     59\u001b[0m     \u001b[39mreturn\u001b[39;00m avg_vector\n\u001b[0;32m     61\u001b[0m \u001b[39m# Vectorize every speech \u001b[39;00m\n\u001b[1;32m---> 62\u001b[0m df_data[\u001b[39m'\u001b[39m\u001b[39mVectorized_Speech\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m=\u001b[39m df_data[\u001b[39m'\u001b[39;49m\u001b[39mSpeech\u001b[39;49m\u001b[39m'\u001b[39;49m]\u001b[39m.\u001b[39;49mapply(tokenize)\n",
      "File \u001b[1;32mc:\\Users\\hp\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pandas\\core\\series.py:4771\u001b[0m, in \u001b[0;36mSeries.apply\u001b[1;34m(self, func, convert_dtype, args, **kwargs)\u001b[0m\n\u001b[0;32m   4661\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mapply\u001b[39m(\n\u001b[0;32m   4662\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[0;32m   4663\u001b[0m     func: AggFuncType,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   4666\u001b[0m     \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs,\n\u001b[0;32m   4667\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m DataFrame \u001b[39m|\u001b[39m Series:\n\u001b[0;32m   4668\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m   4669\u001b[0m \u001b[39m    Invoke function on values of Series.\u001b[39;00m\n\u001b[0;32m   4670\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   4769\u001b[0m \u001b[39m    dtype: float64\u001b[39;00m\n\u001b[0;32m   4770\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m-> 4771\u001b[0m     \u001b[39mreturn\u001b[39;00m SeriesApply(\u001b[39mself\u001b[39;49m, func, convert_dtype, args, kwargs)\u001b[39m.\u001b[39;49mapply()\n",
      "File \u001b[1;32mc:\\Users\\hp\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pandas\\core\\apply.py:1123\u001b[0m, in \u001b[0;36mSeriesApply.apply\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1120\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mapply_str()\n\u001b[0;32m   1122\u001b[0m \u001b[39m# self.f is Callable\u001b[39;00m\n\u001b[1;32m-> 1123\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mapply_standard()\n",
      "File \u001b[1;32mc:\\Users\\hp\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pandas\\core\\apply.py:1174\u001b[0m, in \u001b[0;36mSeriesApply.apply_standard\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1172\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m   1173\u001b[0m         values \u001b[39m=\u001b[39m obj\u001b[39m.\u001b[39mastype(\u001b[39mobject\u001b[39m)\u001b[39m.\u001b[39m_values\n\u001b[1;32m-> 1174\u001b[0m         mapped \u001b[39m=\u001b[39m lib\u001b[39m.\u001b[39;49mmap_infer(\n\u001b[0;32m   1175\u001b[0m             values,\n\u001b[0;32m   1176\u001b[0m             f,\n\u001b[0;32m   1177\u001b[0m             convert\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mconvert_dtype,\n\u001b[0;32m   1178\u001b[0m         )\n\u001b[0;32m   1180\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(mapped) \u001b[39mand\u001b[39;00m \u001b[39misinstance\u001b[39m(mapped[\u001b[39m0\u001b[39m], ABCSeries):\n\u001b[0;32m   1181\u001b[0m     \u001b[39m# GH#43986 Need to do list(mapped) in order to get treated as nested\u001b[39;00m\n\u001b[0;32m   1182\u001b[0m     \u001b[39m#  See also GH#25959 regarding EA support\u001b[39;00m\n\u001b[0;32m   1183\u001b[0m     \u001b[39mreturn\u001b[39;00m obj\u001b[39m.\u001b[39m_constructor_expanddim(\u001b[39mlist\u001b[39m(mapped), index\u001b[39m=\u001b[39mobj\u001b[39m.\u001b[39mindex)\n",
      "File \u001b[1;32mc:\\Users\\hp\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pandas\\_libs\\lib.pyx:2924\u001b[0m, in \u001b[0;36mpandas._libs.lib.map_infer\u001b[1;34m()\u001b[0m\n",
      "Cell \u001b[1;32mIn[21], line 41\u001b[0m, in \u001b[0;36mtokenize\u001b[1;34m(speech)\u001b[0m\n\u001b[0;32m     38\u001b[0m corpus_tokenized \u001b[39m=\u001b[39m [nltk\u001b[39m.\u001b[39mword_tokenize(sentence) \u001b[39mfor\u001b[39;00m sentence \u001b[39min\u001b[39;00m corpus]\n\u001b[0;32m     40\u001b[0m \u001b[39m# Train the Word2Vec model on the tokenized corpus\u001b[39;00m\n\u001b[1;32m---> 41\u001b[0m model \u001b[39m=\u001b[39m Word2Vec(corpus_tokenized, vector_size\u001b[39m=\u001b[39;49m\u001b[39m300\u001b[39;49m, window\u001b[39m=\u001b[39;49m\u001b[39m5\u001b[39;49m, min_count\u001b[39m=\u001b[39;49m\u001b[39m1\u001b[39;49m, workers\u001b[39m=\u001b[39;49m\u001b[39m4\u001b[39;49m)\n\u001b[0;32m     43\u001b[0m \u001b[39m# Extract the word vectors for each word in the corpus\u001b[39;00m\n\u001b[0;32m     44\u001b[0m vectors \u001b[39m=\u001b[39m []\n",
      "File \u001b[1;32mc:\\Users\\hp\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\gensim\\models\\word2vec.py:430\u001b[0m, in \u001b[0;36mWord2Vec.__init__\u001b[1;34m(self, sentences, corpus_file, vector_size, alpha, window, min_count, max_vocab_size, sample, seed, workers, min_alpha, sg, hs, negative, ns_exponent, cbow_mean, hashfxn, epochs, null_word, trim_rule, sorted_vocab, batch_words, compute_loss, callbacks, comment, max_final_vocab, shrink_windows)\u001b[0m\n\u001b[0;32m    428\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_check_corpus_sanity(corpus_iterable\u001b[39m=\u001b[39mcorpus_iterable, corpus_file\u001b[39m=\u001b[39mcorpus_file, passes\u001b[39m=\u001b[39m(epochs \u001b[39m+\u001b[39m \u001b[39m1\u001b[39m))\n\u001b[0;32m    429\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbuild_vocab(corpus_iterable\u001b[39m=\u001b[39mcorpus_iterable, corpus_file\u001b[39m=\u001b[39mcorpus_file, trim_rule\u001b[39m=\u001b[39mtrim_rule)\n\u001b[1;32m--> 430\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtrain(\n\u001b[0;32m    431\u001b[0m         corpus_iterable\u001b[39m=\u001b[39;49mcorpus_iterable, corpus_file\u001b[39m=\u001b[39;49mcorpus_file, total_examples\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcorpus_count,\n\u001b[0;32m    432\u001b[0m         total_words\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcorpus_total_words, epochs\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mepochs, start_alpha\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49malpha,\n\u001b[0;32m    433\u001b[0m         end_alpha\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmin_alpha, compute_loss\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcompute_loss, callbacks\u001b[39m=\u001b[39;49mcallbacks)\n\u001b[0;32m    434\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    435\u001b[0m     \u001b[39mif\u001b[39;00m trim_rule \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\hp\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\gensim\\models\\word2vec.py:1045\u001b[0m, in \u001b[0;36mWord2Vec.train\u001b[1;34m(self, corpus_iterable, corpus_file, total_examples, total_words, epochs, start_alpha, end_alpha, word_count, queue_factor, report_delay, compute_loss, callbacks, **kwargs)\u001b[0m\n\u001b[0;32m   1042\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmin_alpha \u001b[39m=\u001b[39m end_alpha \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmin_alpha\n\u001b[0;32m   1043\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mepochs \u001b[39m=\u001b[39m epochs\n\u001b[1;32m-> 1045\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_check_training_sanity(epochs\u001b[39m=\u001b[39;49mepochs, total_examples\u001b[39m=\u001b[39;49mtotal_examples, total_words\u001b[39m=\u001b[39;49mtotal_words)\n\u001b[0;32m   1046\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_check_corpus_sanity(corpus_iterable\u001b[39m=\u001b[39mcorpus_iterable, corpus_file\u001b[39m=\u001b[39mcorpus_file, passes\u001b[39m=\u001b[39mepochs)\n\u001b[0;32m   1048\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39madd_lifecycle_event(\n\u001b[0;32m   1049\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mtrain\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[0;32m   1050\u001b[0m     msg\u001b[39m=\u001b[39m(\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1054\u001b[0m     ),\n\u001b[0;32m   1055\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\hp\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\gensim\\models\\word2vec.py:1554\u001b[0m, in \u001b[0;36mWord2Vec._check_training_sanity\u001b[1;34m(self, epochs, total_examples, total_words, **kwargs)\u001b[0m\n\u001b[0;32m   1551\u001b[0m     logger\u001b[39m.\u001b[39mwarning(\u001b[39m\"\u001b[39m\u001b[39mEffective \u001b[39m\u001b[39m'\u001b[39m\u001b[39malpha\u001b[39m\u001b[39m'\u001b[39m\u001b[39m higher than previous training cycles\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m   1553\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mwv\u001b[39m.\u001b[39mkey_to_index:  \u001b[39m# should be set by `build_vocab`\u001b[39;00m\n\u001b[1;32m-> 1554\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39myou must first build vocabulary before training the model\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m   1555\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mlen\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mwv\u001b[39m.\u001b[39mvectors):\n\u001b[0;32m   1556\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39myou must initialize vectors before training the model\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "\u001b[1;31mRuntimeError\u001b[0m: you must first build vocabulary before training the model"
     ]
    }
   ],
   "source": [
    "\n",
    "def tokenize(speech):\n",
    "    # Sentence Tokenization\n",
    "    sentence = nltk.sent_tokenize(speech)\n",
    "\n",
    "    # Creating an empty list to store the cleaned dataset \n",
    "    corpus = []\n",
    "\n",
    "    # Loop through each sentence in the speech\n",
    "    for i in range(len(sentence)):\n",
    "        # Remove any non-alphabetic characters in the sentence and convert to lowercase\n",
    "        review = re.sub(\"[^a-zA-Z]\", \" \", sentence[i])\n",
    "        review = review.lower()\n",
    "        \n",
    "        # Tokenize the sentence into a list of words\n",
    "        review = review.split()\n",
    "        \n",
    "        # Lemmatize each word in the sentence and remove stop words\n",
    "        lemmatizer = WordNetLemmatizer()\n",
    "        review = [lemmatizer.lemmatize(word) for word in review if word not in set(stopwords.words(\"english\"))]\n",
    "        \n",
    "        # Convert the list of words back into a sentence\n",
    "        review = ' '.join(review)\n",
    "        \n",
    "        # Add the cleaned sentence to the corpus\n",
    "        corpus.append(review)\n",
    "\n",
    "    # Tokenize each sentence in the corpus into a list of words\n",
    "    corpus_tokenized = [nltk.word_tokenize(sentence) for sentence in corpus]\n",
    "\n",
    "    # Train the Word2Vec model on the tokenized corpus\n",
    "    model = Word2Vec(corpus_tokenized, vector_size=300, window=5, min_count=1, workers=4)\n",
    "\n",
    "    # Extract the word vectors for each word in the corpus\n",
    "    vectors = []\n",
    "    for sentence in corpus_tokenized:\n",
    "        for word in sentence:\n",
    "            if word in model.wv.key_to_index:\n",
    "                # If the word is in the Word2Vec model's vocabulary, extract its vector\n",
    "                word_vector = model.wv.get_vector(word)\n",
    "                vectors.append(word_vector)\n",
    "            else:\n",
    "                # If the word is not in the vocabulary, print a message to the console\n",
    "                print(f\"{word} not in vocabulary.\")\n",
    "    \n",
    "    # Calculate the average vector for the entire speech\n",
    "    vectors_array = np.array(vectors)\n",
    "    avg_vector = np.mean(vectors_array, axis=0)\n",
    "    \n",
    "    return avg_vector\n",
    "\n",
    "# Vectorize every speech \n",
    "df_data['Vectorized_Speech'] = df_data['Speech'].apply(tokenize)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
